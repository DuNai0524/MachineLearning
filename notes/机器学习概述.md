---
tags: 
book: 《神经网络与深度学习》
---
## 基本概念

- 特征：例如水果的颜色、大小、形状等。
- 标签：可以是水果的甜度，也可以是好或坏（即连续指或者是离散值）。
- 样本：标记好的特征与标签。
- 数据集：一组样本的集合。
- 训练集：用来训练模型的。
- 测试集：用来检验模型好坏的。
- 特征向量：一个物品所有特征所构成的向量，即 $x=[x_{1},x_{2,\dots,x_{D}}]^T$
- 学习过程：通过学习算法来寻找最优函数 $f(x)^*$ 的过程。

## 机器学习的三个基本要素

### 模型

机器学习的目标是找到一个**模型**来**近似真实映射函数 $g(x)$ 或真实条件概率分布 $P_{r}(t|x)$**

#### 线性模型

$$
f(x;\theta) = \omega^T + b
$$

其中参数 $\theta$ 包括了权重向量 $\omega$ 和偏置 $b$

#### 非线性模型

$$
f(x;\theta) = \omega^T \phi(x) + b
$$

其中参数 $\theta$ 包括了权重向量 $\omega$ 和偏置 $b$，$\phi(x) = [\phi_{1}(x),\phi_{2}(x),\dots,\phi_{K}(x)]$

若如果 $\phi(x)$ 本身为可学习的基函数，则 $f(x;\theta)$ 等价于**神经网络**模型。

### 学习准则

一个好的模型 $f(x,\theta^*)$ 应该在所有 $(x,y)$ 的可能取值上都与真实映射函数 $y=g(x)$ 一致。

#### 损失函数

损失函数是一种非负实数函数，用来量化模型预测和真实标签之间的差异。

**0-1 损失函数**

$$
\mathcal{L}(y,f(x;\theta)) = \begin{cases}
0 \ if \ y = f(x;\theta) \\ 1 \ if \ y \ne f(x;\theta)
\end{cases}
=I(y \ne f(x;\theta))
$$

-  缺点：数学性质不好；不连续且导数为 0；难以优化。

**平方损失函数**

$$
\mathcal{L}(y,f(x;\theta))=\frac{1}{2}(y-f(x;\theta))^2
$$

- 适用于预测标签 $y$ 为实数值的任务中。
- 缺点：不适用于分类问题。

**交叉熵损失函数**

一般用于分类问题，同时，交叉熵损失函数也是**负对数似然函数**。

$\mathcal{L}(y,f(x;\theta))=-\sum^{C}_{c=1}y_{c \log f_{c}(x;\theta)}$ 或 $\mathcal{L}(y,f(x;\theta)) = -\log f_{y}(x;\theta)$

**Hinge 损失函数**

对于二分类问题，假设 $y$ 的取值为 $\{-1,+1\}$，$f(x;\theta) \in \mathbb{R}$，其函数为：

$$
\mathcal{L}(y,f(x;\theta))=max(0,1-yf(x;\theta)) \stackrel{\triangle}{=}
[1=yf(x;\theta)]_{+}
$$

其中 $[x]_{+} = max(0,x)$

#### 风险最小化准则

- 经验风险最小化准则（ERM）：$\theta^* = \arg _{\theta}\min \mathcal{R}^{emp}_{\mathcal{D}}(\theta)$，其中 $\mathcal{R}^{emp}_{D}(\theta)=\frac{1}{N}\mathcal{L}(y^{(n)},f(x^{(n)};\theta)$
- 结构风险最小化准则（SRM）：$\theta^* = \arg_{\theta} \min \frac{1}{N}\mathcal{L}(y^{(n)},f(x^{(n)};\theta) + \frac{1}{2} \lambda ||\theta||^2$
- 过拟合：模型在训练集上错误率很低，但是在未知数据上错误率很高。
- 欠拟合：模型不能很好的拟合训练数据。

### 优化算法

确定了训练集 $\mathcal{D}$，假设空间 $\mathcal{F}$ 以及学习准则之后，找到最优化模型的问题即为最优化问题。机器学习的训练过程就是最优化问题的求解过程。

- 参数优化与超参数优化。
- 常见超参数：聚类算法中的类别个数、梯度下降法中的步长、正则化项的系数、神经网络的层数、支持向量机的核参数等。

**梯度下降法**

亦为**批量梯度下降法（BGD）**，在机器学习中最简单、最常用的优化算法。其中 $\alpha$ 为学习率。

$$
\theta_{t+1}=\theta_{t}-\alpha \frac{\partial \mathcal{R}_{\mathcal{D}}(\theta)}{\partial \theta}=\theta _{t} - \alpha \frac{1}{N}\sum^{N}_{n=1}\frac{\partial \mathcal{L}(y^{(n)}, f(x^{(n)};\theta))}{\partial \theta}
$$

当训练集中样本的数量很大时，空间复杂度比较高，每次迭代的计算开销也比较大

**提前停止**

每次迭代时，把新得到的模型的参数在验证集上进行测试，并计算错误率，如果在验证集上的错误率不再下降，就停止迭代，这种策略叫**提前停止**。

**随机梯度下降**

随机梯度下降（SGD）相当于在 BGD 上引入了随机噪声

- 优点：实现简单、收敛速度非常快
- 缺点：无法充分利用计算机的并行计算能力

**小批量梯度下降**

$$
\theta_{t+1} \leftarrow \theta_{t} - \alpha \frac{1}{K} \sum_{(x,y) \in S_{t}} \frac{\partial \mathcal{L}(y,f(x;\theta))}{\partial \theta}
$$

- 有点：收敛快、计算开销小
- 主流优化算法

## 机器学习的简单示例——线性回归

### 参数学习

以线性回归为例，来介绍四种不同的参数估计方法

**经验风险最小化**

- 即**最小乘二法**
- $\omega^* = (XX^T)^{-1}Xy$
- $XX^T \in \mathbb{R}^{(D+1) \times (D+1)}$ 必须存在逆矩阵。当不可逆时，可以使用以下方法来计算估计参数：
	- 使用梯度下降
	- 先使用主成分分析等方法处理数据，再使用最小乘二法来估计参数

**结构风险最小化**

- 岭回归，即若特征之间有较大的多重共线性 (矩阵不满秩)，则给 $XX^T$ 的主对角线元素加入一个常数 $\lambda$ 使得矩阵满秩
- $\omega^* = (XX^T + \lambda I)^{-1}Xy$
- $R(\omega)= \frac{1}{2}||y-X^T \omega||^2 + \frac{1}{2}\lambda ||\omega||^2$

**最大似然估计**

- 频率角度
- 最大化似然函数：$p(y|x;\omega,\sigma)=\frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{(y-\omega^T x)^2}{2 \sigma^2})$
- 令 $\frac{\partial \log p(y|X;\omega,\sigma)}{\partial \omega} = 0$，得 $\omega^{ML}=(XX^T)^{-1}Xy$
- 缺点：训练数据较少时会发生过拟合

**最大后验估计**

- 贝叶斯学派
- $\omega^{MAP}=\arg_{\omega} \max p(y|X, \omega ; \sigma)p(\omega;v)$
- 是一种参数的区间估计
- $\log p(\omega|X,y;v,\sigma) \propto \log p(y|X,\omega |\sigma)+ \log p(\omega; v) = - \frac{1}{2\sigma^2}||y-X^T \omega ||^2 - \frac{1}{2v^2}\omega^T\omega$，可以得到MAP等价于平方损失的结构方法最小化；

## 偏差-方差分解

> [!error] Oops!
这一部分涉及到数理统计部分，有点无法理解，等后面补上一点概率论与数理统计知识再回来看

## 机器学习算法的类型

- 监督学习
	- 回归问题
	- 分类问题
	- 结构化学习
- 无监督学习
- 强化学习

## 数据的特征表示

- 图像特征
- 文本特征
	- 词带模型：将文本看作词的集合，不考虑词序信息，不能精准的表示文本信息
	- 改进方式：使用 N 元特征
- 表示学习：让机器自动地学习出有效的特征

### 传统特征学习

#### 特征选择

选取原始特征集合的一个有效子集，使得基于这个特征子集训练出来的模型准确率最高，即保留有用的特征，一处冗余或者无关的特征。

**子集搜索**

- 选择一个最优的候选子集
- 向前搜索：由空集合开始，每一轮添加该轮最优的特征
- 反向搜索：从原始特征集合开始，每次删除最无用的特征
- 子集搜索方法：
	- 过滤式方法：不依赖具体机器学习模型
	- 包裹式方法：使用后续机器学习模型的准确率作为评价来选择一个特征子集

**$\mathcal{l}_{1}$ 正则化**

$\mathcal{l_{1}}$ 正则化会导致系数特征，因此间接实现了特征选择

#### 特征抽取

构造一个新的特征空间，并将原始特征投影在新的空间中得到新的表示。

- 监督方法：抽取对一个特定的预测任务最有用的特征；如：线性判别分析；
- 无监督方法：与任务无关，目标是减少冗余信息和噪声；如：PCA；

### 深度学习方法

- 难点：如何评价表示学习对最终系统输出结果的贡献或影响（贡献度分配问题）

## 评价指标

主要适用于分类问题

**准确率**

最常用的评价指标就是**准确率**，其中 $I(\cdot)$ 为指示函数

$$
\mathcal{A} = \frac{1}{N}I(y^{(n)}=\hat{y}^{(n)})
$$

**错误率**

$$
\varepsilon = 1 - \mathcal{A} = \frac{1}{N}\sum^N_{n=1}I(y^{(n)} \ne \hat{y}^{(n)})
$$

**精确律和召回率**

- 精准率：$\mathcal{P_{C}}=\frac{TP_{c}}{TP_{c}+FP_{c}}$，表示所有预测为类别 $c$ 的样本中预测正确的比例
- 召回率：$\mathcal{R_{c}}=\frac{TP_{c}}{TP_{c}+FN_{c}}$，表示所有真实标签为类别 $c$ 的样本中预测正确的比率
- F 值：$\mathcal{F_{c}}=\frac{(1+\beta^2)\times \mathcal{R_{c}}\times\mathcal{P_{c}}}{\beta^2 \times \mathcal{P_{c}} + \mathcal{R_{c}}}$，表示精确率和召回率的调和平均

其中对于类别 c，模型在测试集上结果可以分为以下四种

- 真正例 TP：样本类别预测正确
- 假负例 FN：样本类别为 c，预测为其他类
- 假正例 FP：样本类别为其他类，预测为 c
- 真负例 TN：样本类别为其他类，预测为其他类

![Pasted image 20250508202631|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/Pasted%20image%2020250508202631.png?imageSlim)

**宏平均与微平均**

为了计算分类算法在所有类别上的总体准确率、召回率和 F1 值，经常使用两种平均方法：

- 宏平均：每一类性能指标的算数平均值

$$
\begin{align}
\mathcal{P_{marco}}=\frac{1}{C}\sum^C_{c=1}\mathcal{P_{c}} \\
\mathcal{R_{marco}}=\frac{1}{C}\sum^C_{c=1}\mathcal{R_{c}} \\
\mathcal{F1_{marco}}=\frac{2 \times \mathcal{P}_{marco} \times \mathcal{R_{marco}}}{\mathcal{P}_{marco} + \mathcal{R_{marco}}}
\end{align}
$$

- 微平均：每一个样本的性能指标的算数平均值，每个样本的精确率和召回率是相同的，要么为1，要么为0。
- 实际应用中，还会用到AUC、ROC、PR曲线、TopN准确率等。

**交叉验证**

把原始数据集平均分为 K 组不重复的子集，每次选择 K-1 组子集作为训练集，剩下的一组作为验证集，可以进行 K 次实验得到 K 个模型，将这 K 个模型在各自验证集上的错误率的平均作为分类器的评价。

## 定理和理论

### PAC 学习理论

- 根据大数定律，当训练集大小|D|趋向于无穷大时，泛化误差趋向于 0，即经验风险趋近于期望风险：$\\lim_{ |D| \to \infty }\mathcal{R}(f)-\mathcal{R^{emp}_{D}(f)}=0$
- PAC 学习理论可以帮助分析一个机器学习方法在什么条件下可以学习到一个近似正确的分类器
- 如果希望模型的假设空间越大，泛化误差越小，其需要的样本数量越多

### 没有免费午餐定理

对于基于迭代的最优化算法，不存在某种算法对所有问题都有效。

### 奥卡姆剃刀定理

简单的模型泛化能力更好，如果有两个性能相近的模型，我们应该选择更简单的模型。

### 丑小鸭定理

- “丑小鸭和白天鹅之间的区别和两只白天鹅之间的区别一样大”；
- 世界上不存在相似性的客观标准，一切相似性的标准都是主观的。

### 归纳偏置

- 学习算法对学习的问题做的一些假设
- 在贝叶斯学习中也经常被称为先验
