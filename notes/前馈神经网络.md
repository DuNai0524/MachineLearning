---
tags:
---
**人工神经网络** (ANN) 是指一系列受生物学和神经科学启发的数学模型. 在人工智能领域，人工神经网络也常常称为**神经网络** (NN) 或**神经模型**.

- 连接主义模型
- 分布式并行处理

主要有 3 个特性：

- 信息是分布式的 (非局部的)
- 记忆和知识是存储在单元之间的连接上
- 通过逐渐改变单元之间的连接强度来学习新的知识

## 神经元

人工**神经元**，是构成神经网络的基本单位, 主要是模拟生物神经元的结构和特性，接收一组输入信号并产生输出。

1943 年，提出了 **MP 神经元**，其中 MP 神经元中的激活函数 $f$ 为 0 或 1 的阶跃函数，而现代神经元中激活函数要求是连续可导的函数。

使用净输入 $z \ \ z \in \mathbb{R}$ 表示一个神经元所获得的输入信号 $x$ 的加权和：

$$
z = w^Tx + b
$$

**激活函数**

激活函数在神经元中非常重要。为了增强网络的表示能力和学习能力，激活函数需要具备以下几个性质：

1. 连续并可导（允许少数点上不可导）的非线性函数。可导的激活函数能直接利用数值优化的方法来学习网络参数。
2. 激活函数导函数要尽可能简单，有利于提高网络计算效率
3. 激活函数的导函数的值域要在一个合适的区间内，不能太大也不能太小，否则会影响训练的效率和稳定性。

### Sigmoid 型函数

Sigmoid 型函数是指一类 S 型曲线函数，为两端饱和函数，常用的 Sigmoid 型函数有 **Logistic 函数和 Tanh 函数**。

**Logistic 函数**

$$
\sigma(x)=\frac{1}{1+\exp(-x)}
$$

值域为 (0,1)，非 0 中心化，使得最后一层的神经元发生偏置偏移，并进一步使得梯度下降的收敛速度变慢。

**Tanh 函数**

$$
\tanh(x)=\frac{\exp(x)-\exp(-x)}{\exp(x) + \exp(-x)}
$$

Tanh 函数可以看作放大并平移的 Logistic 函数，其值域为 (1,1)，并且输出是 0 中心化的。

$$
\tanh(x)=2\sigma(2x)-1
$$

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250513155833302.png?imageSlim)

**Hard-Logistic 函数和 Hard-Tanh 函数**

Logistic 函数和 Tanh 函数都是 Sigmoid 函数，具有饱和性，但是计算开销比较大，因为这两个函数都是在中间（0 附近）近似线性，两端饱和，因此，这两个函数可以通过分段函数来近似。

Hard-Logistic 函数：

$$
\text{hard-logistic(x)}=\begin{cases}
1 \ \ g_{l}(x) \geq 1 \\
g_{l} \ \ 0 < g_{l}(x)<1 \\
0 \ \ g_{l}(x) \leq 0
\end{cases}
= \max(\min(0.25x + 0.5,1),0)
$$

Hard-Tanh 函数：

$$
\text{hard-tanh(x)} = \max(min(x,1), -1)
$$

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250513160349470.png?imageSlim)

### ReLU 函数

ReLU 是目前深度神经网络中经常使用的激活函数，实际上是一个斜坡函数，定义为

$$
\text{ReLU(x)}=\begin{cases}
x \ \ \ \ x \geq 0 \\
0 \ \ \ \ x < 0
\end{cases}
= \max(0,x)
$$

**优点**

- 采用 ReLU 的神经元只需要进行加、乘和比较的操作，计算上更加高效。
- 被认为具有生物学合理性。

**缺点**

- ReLU 函数输出是非 0 中心化的，给后一层的神经网络引入了偏置偏移，影响下降的效率
- 会出现**死亡 ReLU 问题**：在训练时，如果参数在一次不恰当的更新之后，第一个隐藏层中的某个 ReLU 神经元在所有的训练数据上都不能被激活，那么在这个神经元自身参数的梯度都会永远是 0，并且在以后的训练过程中永远不能被激活。

**带泄露的 ReLU**

$$
\text{LeakyReLU(X)}=\begin{cases}
x \ \ \ if \  x > 0 \\
\gamma x \ \ \ if \ x \leq 0
\end{cases}
=\max(0,x) + \gamma \min(0,x)
$$

- $\gamma$ 是一个很小的常数，当 $\gamma < 1$ 时，带泄露的 ReLU 也可以写为 $\text{LeakyReLU}(x)=\max(x, \gamma x)$
- 相当于是一个比较简单的 maxout 单元。

**带参数的 ReLU**

$$
\text{PReLU}_{i}=\begin{cases}
x \ \ \ \ \ \ \ if \ \ x > 0 \\
\gamma_{i}x \ \ \ \ if \ \ x \leq 0
\end{cases}
$$

- 表示第 i 个神经元的 PReLU, 且 PReLU 是非饱和函数。
- 如果 $\gamma_{i}=0$，PReLU 退化成 ReLU
- PReLU 允许不同神经元有不同的参数，也可以一组神经元共享一个参数

**ELU 函数**

$$
\text{ELU}(x)=\begin{cases}
x \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  if \ x > 0 \\
\gamma(\exp(x) - 1) \ \ \ if \ x \leq 0
\end{cases}
$$

**Softplus 函数**

$$
\text{Softplus}(x) = \log(1+\exp(x))
$$

Softplus 函数的导数刚好是 Logistic 函数。虽然也具有单侧抑制、宽兴奋边界的特性，却没有稀疏激活性。

以上几个函数的示例如下图所示：

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250513170001438.png?imageSlim)

### Swish 函数

$$
\text{swish}(x) = x \sigma(\beta x)
$$

- 一种自门控激活函数
- 可以看作线性函数和 ReLU 函数之间的非线性插值函数，其程度由参数 $\beta$ 控制

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250513170326810.png?imageSlim)

### GELU 函数

$$
\text{GELU}=xP(X \leq x)
$$

- 是一种通过门控机制来调整其输出值的激活函数，和 Swish 函数类似
- 其中 $P(X \leq x)$ 是高斯分布 $\mathcal{N}(\mu,\sigma^2)$ 的累计分布函数，其中 $\mu$，$\sigma$ 为超参数，一般设置为 $\mu=0,\sigma=1$ 即可。
- 由于高斯分布的累计分布函数为 S 型函数因此 GELU 函数可以用 Tanh 函数或者 Logistic 函数来近似

### Maxout 单元

$$
\text{maxout}(x)=\underset{k \in[1,K]}{\max}(z_{k})
$$

- 不同于 Sigmoid、ReLU 函数的输入，是一个标量；Maxout 单元的输入是上一层神经元的所有全部原始输出，是一个向量 $x=[x_{1};x_{2};\dots;x_{D}]$。
- 每个 Maxout 单元有 K 个权重向量 $w_{k} \in \mathbb{R}^D$ 和偏置 $b_{k} (1\leq k\leq K)$。对于输入 $x$，可以得到 K 个净输入 $z_{k} = w^T_{k} + b$。
- 特点：Maxout 单元不单是净输入到输出之间的非线性映射，而是整体学习输入到输出之间的非线性映射关系。Maxout 函数可以看作任意凸函数的分段线性近似，并且在有限的点上是不可分的。

## 网络结构

### 前馈网络

前馈网络可以看作一个函数，通过简单非线性函数的多次符合，实现输入空间到输出空间的复杂映射，这种网络结构简单，容易实现。CNN 就是一个前馈网络。

### 记忆网络

- 记忆网络可以看作一个程序，拥有更强的计算和记忆能力。
- 网络中神经元不但可以接收其他神经元的信息，也可以接收自己的历史信息。在不同的时刻具有不同的状态，可以单向也可以双向传播。
- 为了增强记忆网络的记忆容量，可以引入外部记忆单元和读写机制，用来保存网络中的一些状态，称为记忆增强神经网络。
- RNN 就是一个典型的记忆网络

### 图网络

- 定义在图数据结构上的神经网络，图中每一个节点都由一个或者一组神经元组成。
- 节点之间可以是有向的，也可以是无向的，每个节点都可以收到来自相邻节点或者自身的信息。
- 包含 GCN，GAT，MPNN 等

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250513173731693.png?imageSlim)

## 前馈神经网络

- 通常也被称为**多层感知器**（实际上是由多层 Logistic 回归模型组成）。
- 前馈神经网络示例以及符号：

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250513213536633.png?imageSlim)

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250513213554656.png?imageSlim)

令 $a^{(0)}=x$，前馈神经网络通过不断迭代下面的公式来进行信息传播：

$$
\begin{gathered}
z^{(l)}=W^{(l)}a^{(l-1)} + b^{(l)} \\
a^{(l)} = f_{l}(z^{(l)})
\end{gathered}
$$

首先根据第 l-1 层神经元的活性值 $a^{(l-1)}$ 计算出第 l 层神经元的净活性值 $z^{(l)}$，然后经过一个激活函数得到第 l 层神经元的活性值。因此，我们也可以把每个神经层看作一个仿射变换。

根据上面的公式，FNN 通过逐层信息传递，得到网络最后的输出 $a^{(L)}$，整个网络可以看作一个复合函数 $\phi(x;W,b)$，将向量 x 作为第 1 层的输入 $a^{(0)}$，将第 L 层的输出 $a^{(L)}$ 作为整个函数的输出。

### 通用近似定理

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250513214629631.png?imageSlim)

- 常见的连续非线性函数都可以用前馈神经网络来近似。
- 但是，通用近似定理知识说明了神经网络的计算能力可以去近似一个给定的连续函数，但并没有给出如何找出这样一个网络，以及是否是最优的

### 应用到机器学习

多层前馈神经网络可以看作一个非线性复合函数 $\phi : \mathbb{R}^D \to \mathbb{R}^{D'}$，而机器学习的特征提取是将原始特征向量 x 转换到更有效的特征向量 $\phi(x)$。

### 参数学习

- 损失函数：$\mathcal{L}(y,\hat{y})=-y^T \log \hat{y}$
- 结构化风险函数：$\mathcal{R}(W,b)=\frac{1}{N}\sum^N_{n=1}\mathcal{L}(y^{(n)},\hat{y}^{(n)})+\frac{1}{2}\lambda||W||^2_{F}$
- 更新方式：

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250513220930150.png?imageSlim)

## 反向传播算法

梯度下降法需要计算损失函数对参数的偏导数，如果通过链式法则逐一对每个参数进行求偏导比较低效，在神经网络训练中经常使用反向传播算法来高效的计算梯度。

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250513221318617.png?imageSlim)

(1) 计算偏导数 $\frac{\partial z^{(l)}}{\partial w^{(l)}_{ij}}$

> [!note]
> 这里矩阵微分采用的是分母布局，即一个列向量关于标量的偏导数是行向量

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250513221443448.png?imageSlim)

(2) 计算偏导数 $\frac{\partial z^{(l)}}{\partial b^{(l)}}$

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250513221628050.png?imageSlim)

(3) 计算偏导数 $\frac{\partial \mathcal{L}(y,\hat{y})}{\partial z^{(l)}}$

这个偏导数表示第 l 层神经元对最终损失的影响，也反映了最终损失对第 l 层神经元的敏感程度，因此一般称为第 l 层的误差项，用 $\delta^{(l)}$ 来表示，其间接反映了不同神经元对网络能力的贡献程度，从而比较好的解决了贡献度分配问题 (CAP)。

$$
\delta^{(l)} \triangleq \frac{\partial \mathcal{L}(y,\hat{y})}{\partial z^{(l)}} \ \ \ \ \in \mathbb{R}^{M_{l}}
$$

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250513222707953.png?imageSlim)

从上面公式可以看出，第 l 层的误差项可以通过第 l+1 层的误差项计算得到，这就是误差的**反向传播**。

含义：第 l 层的一个神经元的误差项（或敏感性）是所有与该神经元相连的第 l+1 层的神经元的误差项的权重和，然后，在呈上该神经元激活函数的梯度。

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250513223045262.png?imageSlim)

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250513223108500.png?imageSlim)

训练过程如下：

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250513223128736.png?imageSlim)

## 自动梯度计算

### 数值微分

数值微分指的是使用数值方法来计算函数 $f(x)$ 的导数：

$$
f'(x)=\lim_{ \triangle x \to 0 }\frac{f(x + \triangle x) - f(x)}{\triangle x} 
$$

可能会引起的问题：

- 可能会引起数值计算问题，如舍入误差与截断误差
- 计算复杂度比较大

### 符号微分

符号微分是一种基于符号计算的自动求导方法。符号计算也叫作代数计算，指的是用计算机来处理带有变量的数学表达式，一般不需要代入具体的值。

- 处理对象：数学表达式；
- 优点：符号计算与平台（CPU、GPU）无关；
- 缺点：
	- 编译时间较长，特别是对于循环；
	- 需要设计专门的语言来表示数学表达式，并且要对变量进行预先声明；
	- 很难对程序进行调试；

### 自动微分

自动微分 (AD) 是一种可以对一个 (程序) 函数进行计算导数的方法。处理对象是一个函数或者一段程序：

微分过程如下图所示：

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250513224143902.png?imageSlim)

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250513224155749.png?imageSlim)

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250513224213434.png?imageSlim)

根据计算顺序，有**前向模式**和**反向模式**。

- 前向模式：按计算图中相同的方向来递归地计算梯度
- 反向模式：按计算图中相反的方向来递归地计算梯度
	- 反向模式和反向传播计算梯度的方式相同

- 静态计算图和动态计算图：按计算图构建方式划分
- 符号微分和自动微分

## 优化问题

一般是：非凸优化问题与梯度消失问题。

## 总结

常用激活函数以及导数：

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250513224658349.png?imageSlim)
