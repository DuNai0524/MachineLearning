---
tags:
  - 深度学习
---
我们可以借鉴人脑解决信息过载的机制，从两方面来提高神经网络处理信息的能力：

- 注意力，通过自上而下的信息选择机制来过滤掉大脑的无关信息
- 引入额外的外部记忆，优化神经网网络的记忆结构来提高神经网络存储信息的容量

## 神经学中的注意力

一般注意力有两种：

1. 自上而下的有意识的注意力，称为聚焦式注意力。聚焦式注意力是指有预定目的、依赖任务的，主动有意识地聚焦于某一对象的注意力。
2. 自下而上无意识的注意力，称为基于显著性的注意力。基于显著性的注意力是由外接刺激驱动的注意力，不需要主动干预，也和任务无关。

> [!info]
> 一般来说在深度学习中，我们说的注意力基本上都是**聚焦式注意力**。

## 注意力机制

在计算能力有限的情况下，注意力机制作为一种资源分配方案，将有限的计算资源用来处理更重要的信息，是解决信息超载问题的主要手段。当神经网络用来处理大量的输入信息时，我们也可以借鉴人脑的注意力机制，只选择一些关键的信息来进行处理，来提高神经网络的效率。

- 最大汇聚 (Max Pooling)、门控机制都可以看作自下而上的基于显著性的注意力机制。
- 自上而下的聚焦型注意力也是一种有效的信息选择方式。

注意力机制的计算可以分为两步：

1. 在所有输入信息上计算注意力分布
2. 根据注意力分布来计算输入信息的加权平均

**注意力分布**

为了从 N 个输入向量 $[x_{1}, \dots, x_{N}]$ 中选择出和某个特定任务相关的信息，我们需要引入一个和任务相关的表示，称为**查询向量**，并通过一个打分函数来计算每个**输入向量**和**查询向量**之间的相关性。

给定一个查询向量 $q$，使用注意力 $z$ 来表示被选择信息的索引未知，使用一种“软性”的信息选择机制。在给定 $q$ 和 $X$ 下，选择第 n 个输入向量的概率 $\alpha_{n}$ 为：

$$
\begin{equation}
\begin{split}
\alpha_{n} &= p(z=n|X,q) \\
&= \text{softmax}(s(x_{n},q)) \\
&= \frac{\exp(s(x_{n, q}))}{\sum^N_{j=1}\exp(s(x_{j},q))}
\end{split}
\end{equation}
$$

其中 $\alpha_{n}$ 称为注意力分布，$s(x,q)$ 称为打分函数。打分函数有以下几种方式：

- 加性模型：$s(x,q)=v^{T} \tanh (Wx + Uq)$
- 点积模型：$s(x,q)=x^Tq$
- 缩放点积模型：$s(x,q)=\frac{x^Tq}{\sqrt{ D }}$
- 双线性模型：$s(x,q)=x^T Wq$

- 点积模型在实现上可以更好地利用矩阵乘积，从而使得计算效率更高。
- 缩放点积模型能很好解决 Softmax 函数梯度比较小的问题（输入向量维度比较高的情况）
- 双线性模型在计算相似度时引入了非对称性

**加权平均**

注意力分布 $\alpha_{n}$ 可以解释为在给定任务相关的查询 $q$ 时，第 $n$ 个输入向量受关注的程度。

$$
\begin{equation}
\begin{split}
att(X, q) &= \sum^N_{n=1} \alpha_{n} x_{n} \\
&= \mathbb{E}_{z \backsim p(z|X,q)}[X_{z}]
\end{split}
\end{equation}
$$

以上公式也称为软性注意力机制。注意力机制可以单独使用，但更多地用作神经网络中的一个组件。

![image.png|300](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250525103902076.png?imageSlim)

### 注意力机制的变体

#### 硬性注意力

- 硬性注意力只关注某一个输入向量
- 有两种实现方式：

1. 选取最高概率的一个输入向量：

$$
att(X,q) = x_{\hat{n}} \ \ \ \ \ \ \ \ \ \ \hat{n} = \overset{N}{\underset{n=1}{\arg \max}} \alpha_{n}
$$

2. 通过在注意力分布式上随机采样的方式实现

硬性注意力的缺点：

- 基于最大或随机采样的方式来选择信息，使得最终的损失函数与注意力分布之间的函数关系不可导，无法使用反向传播算法进行计算
- 硬性注意力通常使用反向传播算法进行实现
- 一般使用软性注意力代替硬性注意力

#### 键值对注意力

使用 $(K,V)=[(k_{1},v_{1}),\dots,(k_{N},v_{N})]$ 来表示 $N$ 组输入信息，给定任务相关的查询向量 $q$ 时，注意力函数为：

$$
\begin{equation}
\begin{split}
att((K,V),q) &= \sum^N_{n=1} \alpha_{n} v_{n} \\
&=\sum^N_{n=1} \frac{\exp (s(k_{n},q))}{\sum_{j}\exp(s(k_{j},q))}
\end{split}
\end{equation}
$$

其中 $K=V$ 时，键值对模式就相当于普通的注意力机制。

![image.png|300](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250525110203449.png?imageSlim)

#### 多头注意力

利用多个查询 $Q=[q_{1},\dots,q_{M}]$，并行地从输入信息中选取多组信息，每个注意力关注输入信息的不同部分：

$$
att((K,V),Q) = att((K,V),q_{1}) \oplus \dots \oplus att((K,V),q_{2})
$$

#### 结构化注意力

- 如果输入信息具有层次结构，可以使用层次化的注意力来进行更好的信息选择。
- 还可以假设注意力为上下文相关的二项分布，用一种图模型来构建更加复杂的结构化注意力分布。

#### 指针网络

- 指针网络是一种序列序列模型，将注意力分布作为一个软性的指针来指出相关信息的位置。
- 这里的输出序列是输入序列的下标。

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250525114843795.png?imageSlim)

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250525114901176.png?imageSlim)

图示如下：

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250525114920050.png?imageSlim)

## 自注意力模型

- 建立输入序列之间的长距离依赖关系，使用全连接网络，同时利用注意力机制来“动态”地生成不同连接的权重。
- 经常采用 **查询-键-值** 模式。

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250525115436071.png?imageSlim)

**自注意力模型计算过程**

假设输入序列 $X=[x_{1},\dots,x_{N}] \in \mathbb{R}^{D_{x} \times N}$，输出序列为 $H=[h_{1},\dots,h_{N}]$。

1. 对于每个输入 $x_{i}$，我们首先将其线性映射到三个不同的空间，得到查询向量 $q_{i}$，键向量 $k_{i}$，值向量 $v_i$。
2. 对于每一个查询向量 $q_{n} \in Q$，利用键值对注意力机制，可以得到输出向量 $h_{n}$。

$$
\begin{equation}
\begin{split}
h_{n} &= att((K, V), q_{n}) \\
&= \sum^N_{j=1}\text{softmax}(s(k_{j}, q_{n}))v_{j}
\end{split}
\end{equation}
$$

如果使用缩放点积来作为注意力打分函数，输出向量序列可以简写为：

$$
H = V \text{softmax}(\frac{K^T Q}{\sqrt{ D_{k} }})
$$

全连接模型和自注意模型对比如下图所示：

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250525154605127.png?imageSlim)

- 自注意力模型可以作为神经网络中的一层来使用，可以替换卷积层和循环层，也可以交替使用
- 自注意力模型可以扩展为多头注意力模型

## 人脑中的记忆

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250525155214383.png?imageSlim)

## 记忆增强网络

- 外部记忆(或记忆增强网络)：为了增强网络容量，可以引入辅助记忆单元，将一些和任务相关的信息保存在辅助记忆中，在需要时再进行读取。

典型结构如图所示，一般以下几个模块构成：

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250525160557665.png?imageSlim)

1. 主网络 C：也称为控制器，负责信息处理，外界的交互。
2. 外部记忆单元 M：用来存储信息。
3. 读取模块 R：根据主网络生成的查询向量，从外部记忆单元中读取相应的数据
4. 写入模块 W：根据主网络生成的查询向量和要写入的信息来更新外部记忆。

> [!note]
> 这种结构化的外部记忆是带有地址的，即每个记忆片段都可以按照地址读取和写入。若要实现类似于人脑的联想能力，就需要按内容寻址的方式进行定位，然后进行读取和写入操作。

外部记忆从记忆结构，读写方式等方面可以演变出很多模型：

### 端到端记忆网络

采用一种可微的网络结构，可以多次从外部记忆中读取信息，在端到端记忆网络中，外部记忆单元是只读的。

给定一组需要存储的信息 $m_{1} = \{ m_{1},\dots,m_{N}\}$, 首先将其转换成两组记忆片段 $A$ 和 $C$，分别存放在两个外部记忆单元中，其中 A 用来寻址，C 用来输出。主网络根据输入的 x 生成 q，并使用键值对注意力机制来从外部记忆中读取相关信息 r：

$$
r = \sum^N_{n=1} \text{softmax} (a^T_{n}q)c_{n}
$$

并产生输出

$$
y = f(q+r)
$$

其中 $f(\cdot)$ 为预测函数，当应用到分类任务中时，可以设置为 Softmax 函数。

**多跳操作**

为了实现更复杂的计算，我们可以让主网络和外部记忆进行舵轮交互。

$$
q^{(k)} = r^{(k-1)} + q^{(k-1)}
$$

假设第 k 轮交互的外部记忆为 A 和 C，则主网络从外部记忆读取的信息为：

$$
r^{(k)} = \sum^N_{n=1} \text{softmax}((a_{n}^{(k)})^T q^{(k)})C_{n}^{{(k)}}
$$

- 多跳操作中的参数一般是共享的。为了简化期间，每轮交互的外部记忆也可以共享使用。

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250525164249729.png?imageSlim)
### 神经图灵机

**图灵机**

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250525164406619.png?imageSlim)

组成部分：

1. 一条无限长的纸带
2. 一个符号表
3. 一个读写头
4. 一个状态寄存器
5. 一套控制规则

**神经图灵机**

- 主要由两个部件构成：控制器和外部记忆
- 外部记忆为一个矩阵 $M \in \mathbb{R}^{D \times N}$，其中 D 是每个记忆片段大小，N 是记忆片段数量; 外部记忆可写。
- 控制器为一个 FNN 或者 RNN

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250525165222224.png?imageSlim)

**读操作**

根据记忆力分布，可以计算读向量作为下一个时刻控制器的输入：

$$
r_{t} = \sum^N_{{n=1}} \alpha_{n} m_{t,n}
$$

**写操作**

外部记忆写操作可以分解为两个子操作：增加和删除

首先，控制器产生删除向量和增加向量，分别为要从外部记忆中删除的信息和要增加的信息。删除操作是根据注意力分布来按比例地在每个记忆片段中删除 et，增加操作是根据注意力分布来按比例地给每个记忆片段加入 at：

$$
m_{t+1, n} = m_{t,n}(1-\alpha_{t, n}e_{t}) + \alpha_{t,n}a_{t}
$$

## 基于神经动力学的联想记忆

- 联想记忆模型：主要是通过神经网络的动态演化来进行联想

两种应用场景：

1. 自联想模型：输入的模式和输出的模式在同一空间
2. 异联想模型：输入的模式和输出的模式不在同一空间

### Hopfield 网络

Hopfield 网络是一种循环神经网络模型，由一组相互连接的神经元组成。可以认为是所有神经元都互相连接哦的不分层的神经网络，每个神经元既是输入单元，又是输出单，没有隐藏神经元。一个神经元与自身没有反馈相连，不同神经元之间的连接权重是对称的。

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250525172830964.png?imageSlim)

第 i 个神经元的更新规则为：

$$
s_{i} =
\begin{cases}
+1 \ \ \ \ if  \ \ \ \sum^M_{j=1} w_{ij}s_{j} +b_{i} \geq 0 \\
-1 \ \ \ \ otherwise
\end{cases}
$$

连接权重 $w_{i,j}$ 有以下性质：

$$
\begin{gather*}
w_{ii} = 0 \ \ \ \ \ \ \forall i  \in [1,M] \\
w_{ij} = w_{ij} \ \ \ \ \ \ \forall i,j \in [1,M]
\end{gather*}
$$

- Hopfield 网络的更新可以分为异步和同步两种方式。
- 异步更新是指每次更新一个神经元，神经元的更新顺序可以使随机或者事先固定的。
- 同步更新是指一次更新所有的神经元，需要有一个时钟来进行同步

**能量函数**

- 能量：给每个不同的网络状态定义一个标量属性

$$
E = -\frac{1}{2} s^TWs - b^Ts
$$

- 能量函数怒经过多次迭代之后会达到收敛状态
- 吸引点：给定一个外部收入，网络经过演化达到的某个稳定状态。通常有多个吸引点，每个吸引点为一个能量的局部最优解。

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250525175119181.png?imageSlim)

**联想记忆**

Hopfield的检索是基于内容寻址的检索，具有联想记忆.

**信息存储**

信息存储指的是将意足协向量 $x_{1},\dots,x_{N}$ 存储在网络中的过程 1. 存储过程主要是调整神经元之间的连接权重，因此可以看作一种学习过程。

赫布规则：如果两个神经元经常同时激活，则他们之间的连接加强；如果两个神经元不同时激活，则连接消失。

**存储容量**

Hopfield 网络的最大容量是 0.14M；玻尔兹曼机的容量为 0.6M，但是学习效率较低，需要非常长的事件的演化才能达到均衡状态。

通过改进网络结构，学习方式以及引入更复杂的运算，可以有效改善联想记忆网络的容量。

### 使用联想记忆增加网络容量

- 和结构化的外部记忆相比，联想记忆具有更好的生物学解释性
- 联想记忆都是作为一个更大的网络的组件，用来增加短期记忆的容量
- 联想记忆组件的参数可以使用 Hebbian 方法来进行学习，也可以作为整个网络的一部分来进行学习

