---
tags:
  - 深度学习
---
- 无监督学习主要是指从无标签的数据中学习出一些有用的模式
- 一般直接从原始数据中进行学习
- 如果是建立输入-输出之间的映射关系，那么无监督学习就是发现隐藏的数据中有价值的信息

典型的无监督学习可以分为以下几类：

1. 无监督特征学习
2. 概率密度学习
3. 聚类

最常用的无监督学习准则：最大似然估计，最小重构错误

## 无监督特征学习

无监督特征学习是指从无标注的数据中自动学习有效的数据表示，从而能够帮助后续的机器学习模型更快速地达到更好的性。无监督特征数学主要方法有主成分分析，稀疏编码，自编码器等。

### 主成分分析

- 是一种常用的数据降维方法，使得在转换后的空间中数据的方差最大。

![image.png|300](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250526155653088.png?imageSlim)

相关公式推导可以参考视频:

[【机器学习】【白板推导系列】](https://www.bilibili.com/video/BV1aE411o7qd/?spm_id_from=333.1007.top_right_bar_window_custom_collection.content.click&vd_source=e7b1a81d1c098e4ea95d58f1714c2400)

 主成分分析是一种无监督学习方法，可以作为监督学习的数据预处理方法，用来去除噪声或减少特征之间的相关性。但是它并不能保证投影后数据的类别可分性会更好。一般使用监督学习方法来提高两类可分性。

### 稀疏编码

- 在生物学中，外部信息在编码之后仅有一小部分神经元激活，即外界刺激在视觉神经系统的表示具有很高的稀疏性。编码的稀疏性在一定程度上符合生物学的低功耗特性。

- 线性编码：指的是给定一组基向量 $A=[a_{1},\dots,a_{M}$，将输入样本 $x \in \mathbb{R}^D$ 表示为这些基向量的线性组合：

$$
\begin{equation}
\begin{split}
x &= \sum^M_{m=1} z_{m}a_{m} \\
&= Az
\end{split}
\end{equation}
$$

- 编码的关键是找到一组“完备”的基向量 A，比如主成分分析等。但是主成分分析得到的编码通常是稠密向量，没有稀疏性。

> [!note] 关于完备性
> ![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250526173648560.png?imageSlim)

为了得到稀疏的编码，我们需要找到一组“过完备”的基向量（即 M > B）来进行编码。在过完备基向量之间往往会存在一些冗余性，因此对于一个输入样本，会存在很多有效的编码。如果加上稀疏性限制，就可以减少解空间的大小，得到“唯一”的稀疏编码。目标函数定义如下;

$$
\mathcal{L}(A,Z)=\sum^N_{n=1}(||x^{(n)}-Az^{(n)}||^2 + \eta \rho(z^{(n)}))
$$

其中 $\rho(\cdot)$ 是一个稀疏性衡量函数，$\eta$ 是一个超参数，用来控制稀疏度的强度。z 越稀疏，函数值越小。

稀疏性衡量函数的选择：

- l0 范数（最直接）：$\rho(z) = \sum^M_{m=1} I(|z_{m}| > 0)$
- l1 范数（最常用）：$\rho(z) = \sum^M_{m=1}|z_{m}|$
- 对数函数：$\rho(z) = \sum^M_{m=1}\log(1+z^2_{m})$
- 指数函数：$\rho(z)=\sum^M_{m=1 -\exp(-z^2_{m})}$

#### 训练方法

一般使用交替优化的方法进行：

1. 固定基向量 A，对于每个输入 x，计算其对应的最优编码
2. 固定上一步得到的编码 z，计算其最优的基向量

#### 稀疏编码的优点

- 每一维都可以看作一种特征
- 可以极大地降低计算量
- 可以更好的描述其特征，便于理解
- 可以实现特征的自动选择

### 自编码器

自编码器是通过无监督的方式来学习一组数据的有效编码（或有效表示）。

自编码器结构如下：

- 编码器 f：$\mathbb{R}^D \to \mathbb{R}^M$
- 解码器 g：$\mathbb{R}^M \to \mathbb{R}^D$

自编码器的学习目标是最小化重构错误，通过最小化重构错误，可以有效地学习网络的参数：

$$
\begin{equation}
\begin{split}
\mathcal{L} &= \sum^N_{n=1} ||x^{(n)} - g(f(x^{(x)}))|| ^2 \\
&= \sum^N_{n=1} ||x^{(n)} - f \circ g(x^{(n)})||^2
\end{split}
\end{equation}
$$

- M(特征空间维度)<D (原始空间维度)：一种降维或者特征抽取方法。
- M $\geq$ D，一定可以找到一组或者多组解使得 $f \ \circ \ g$ 为单位函数，并使得重构错误为 0。
	- 加上一些约束，可以得到一些有意义的解。

- 最简单的自编码器：两层全连接神经网络：

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250526191950118.png?imageSlim)

使用自编码器主要是为了得到有效的数据表示，因此在训练结束之后，我们一般会去掉解码器，只保留编码器，编码器的输出可以直接作为后续机器学习模型的输入。

### 稀疏自编码器

- 稀疏自编码器：除了可以学习低维编码之外，也可以学习高维的稀疏编码。
- 优点：具有很高的可解释性，并且同时进行了隐式的特征选择。

通过给自编码器中隐藏单元加上稀疏性限制，自编码器可以学习到数据中一些有用的结构，其目标函数为：

$$
\mathcal{L}=\sum^N_{n=1} || x^{(n)} - x'^{(n)}||^ + \eta \rho(Z)  \lambda||W||^2
$$

- 其中稀疏性度量函数 $\rho(Z)$ 可以使用之前提到的公式，分别计算每个编码的稀疏度，然后进行求和。
- 还可以定义为一组样本中每一个神经云激活的概率

隐藏层第 j 个神经元的平均活性值为：

$$
\hat{\rho}_{j} = \frac{1}{N} \sum^N_{n=1} z_{j}^{(n)}
$$

使用 KL 距离度量 $\hat{\rho}_{j}$ 与 $\rho*$ 之间的差异：

$$
\text{KL}(\rho*||\hat{\rho}_{j})=\rho* \log \frac{\rho*}{\hat{\rho}_{j}} + (1-\rho*) \log \frac{1 - \rho*}{1 - \hat{\rho}_{j}}
$$

### 堆叠自编码器

- 使用逐层堆叠的方式来训练一个深层的自编码器
- 堆叠自编码器一般可以采用逐层训练来学习网络参数

### 降噪自编码器

- 一种引入噪声来增加编码鲁棒性的自编码器，并提高模型的泛化能力。

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250526193414926.png?imageSlim)

## 概率密度估计

基于观测样本来估计一个随机变量的概率密度函数。

### 参数密度估计

根据先验知识假设随机变量服从某种分布，然后通过训练样本来估计分布的参数。

假设一个样本服从一个概率放那儿补函数，其对数自然函数为：

$$
\log p(\mathcal{D};\theta) = \sum^N_{n=1} \log p(x^{(n)};\theta)
$$

使用最大似然估计 (MLE) 来寻找参数 $\theta$ pshide1 $\log p(D;\theta)$ 最大，这样参数估计问题转换成最优化问题：

$$
\theta^{ML} = \overset{N}{\underset{n=1}{\arg \max}} \sum^N_{n=1} \log p(x^{(n)};\theta)
$$

#### 正态分布

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250526194542475.png?imageSlim)

#### 多项分布

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250526194650453.png?imageSlim)

在实际应用中，参数密度估计存在的问题：

- 模型选择问题：如何选择数据分布的密度函数
- 不可观测变量问题
- 维度灾难问题

### 非参数密度估计

- 不假设数据服从某种分布，通过将样本空间划分为不同的区域并估计每个区域的概率来近似数据的概率密度函数。

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250526194853374.png?imageSlim)

在实践中，非参数密度估计一般使用两种方式：

1. 固定区域大小 V，统计落入不提供区域的数量，包括直方图方法和核方法。
2. 改变区域大小使得落入每个区域的样本数量为 K，称为 K 邻居方法。

#### 直方图方法

- 是一种非常直观的估计连续变量密度函数的方法，可以表示为一种柱状图。
- 通常用来处理低维变量
- 缺点是很难扩展到高维变量
- 需要的样本数量会随着维度 D 的增加而指数增加，从而导致维度灾难

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250526195209717.png?imageSlim)

#### 核方法

- 一种直方图的改进方式

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250526195252339.png?imageSlim)

#### K 邻近方法

- 设置一种可变宽度的区域，使得落入每个区域中的样本数量为固定的 K，然后根据非参数密度估计的公式，就可以计算出点 x 的密度。也称为 K 邻近算法。
- K 的选择十分关键，如果 K 太小，无法估计密度函数；K 太大，使得局部密度不准确，并且增加计算开销】
- 也常用于分类问题，称为 K 邻近分类器，K=1 时称为最近邻近分类器
	- 最近邻近分类器性质：$N \to \infty$ 时分类错误率不超过最优分类器错误率的两倍。



