---
tags:
  - 深度学习
---
本章主要介绍自回归生成模型：在深度学习的应用中，有很多数据是以序列的形式存在的。为了有效的描述自然语言规则，我们可以从统计的角度来进行建模。使用哪一个词相当于一个随机时间，一个文本序列的概率大小可以用来评估它符合自然语言规则的程度。

给定一个序列样本 $x_{1:T}=x_{1},x_{2},\dots,x_{T}$，其概率是 T 个词的联合概率：

$$
\begin{equation}
\begin{split}
p(x_{1:T}) &\triangleq P(X_{1:T} = x_{1:T}) \\
&= P(X_{1} = x_{1}, X_{2} = x_{2},\dots,X_{T} = x_{T})
\end{split}
\end{equation}
$$

概率序列模型有两个基本问题：

1. 概率密度估计：给定一组序列数据，估计这些数据背后的概率分布
2. 样本生成：从已知的序列分布中生成新的序列样本

## 序列概率模型

序列数据的特点：

1. 样本是变长的
2. 样本空间非常大

因此，我们很难使用已知的概率模型来直接建模整个序列的概率。

给定一个包含 N 个数据的数据集 $\mathcal{D}=\{x_{1:T_{n}}^{(n)}\}^N_{n=1}$，**概率模型的目标**是需要学习一个模型 $p_{\theta}(x|x_{1:_{t-1}})$ 来最大化整个数据集的对数似然函数，即：

$$
\max_{\theta} \sum_{n=1}^{N} \log p_{\theta}(\mathbf{x}_{1:T_n}^{(n)}) = \max_{\theta} \sum_{n=1}^{N} \sum_{t=1}^{T_n} \log p_{\theta}(\mathbf{x}_t^{(n)} | \mathbf{x}_{1:(t-1)}^{(n)})
$$

在这种序列模型方式中，每一步都需要将前面的输出作为当前步的输入，是一种自回归的方式，因此这一类模型也称为**自回归生成模型**。

现今主流的自回归生成模型有两种：N 元统计模型和深度序列模型

### 序列生成

一旦通过了最大似然估计训练了模型 $p_{\theta}(x|\hat{x}_{1:(t-1)})$ 就可以通过时间顺序来生成一个完整的序列样本。

自回归的方式可以生成一个无限长度的序列。为了避免这种情况，通常会设置一个特殊的符号 `<EOS>` 来表示序列的结束。在训练时，每个序列样本的结尾都加上符号 `<EOS>`，在测试时，一旦生成了符号 `<EOS>`，就中止生成过程。

- 束搜索：一种常用的减少搜索错误的启发式方法。在每一步中，生成 K 个最可能的前缀序列，其中 K 为束的大小，是一个超参数。
- 当使用自回归模型生成一个最可能的序列时，生成过程是一种从左到右的贪婪式搜索过程，在每一步都生成最可能的词。这种贪婪式的搜索方法是次优的，生成的序列并不保证全局最优。

束搜索的过程如下：

在第 1 步的时候，生成 K 个最可能的词；在后面的每一步哦中，从 $K|V|$ 个候选输出中选择 K 个最可能的序列。

束的大小 K 越大，束搜索的复杂度越高，但越有可能生成最优序列。在实际应用中，束搜索可以通过调整束大小 K 来平衡计算复杂度和搜索质量之间的优先级。

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250605094230847.png)

##  N 元统计模型

由于数据稀疏问题，当 t 比较大时，依然很难估计条件概率 $p(x_{t}|x_{1:(t-1)})$。一个简化的方法是使用 N 元模型，假设每个词 $x_{t}$ 只依赖于其前面的 N-1 个词，即：

$$
p(x_t \mid \mathbf{x}_{1:(t-1)}) = p(x_t \mid \mathbf{x}_{(t-N+1):(t-1)}).

$$


当 N=1 时，称为一元模型；当 N=2 时，称为二元模型，以此类推。

**N元模型**

N 元模型中的条件概率可以通过最大似然函数得到：

> [!note] 
> 推导见书 P368

$$
p(x_t \mid \mathbf{x}_{(t - N + 1):(t - 1)}) = 
\frac{\mathrm{m}(\mathbf{x}_{(t - N + 1):t})}{\mathrm{m}(\mathbf{x}_{(t - N + 1):(t - 1)})},
$$

其中 $\mathrm{m}(\mathbf{x}_{(t - N + 1):t})$ 为 $x_{(t-N+1):(t-1)}$ 在数据集中出现的次数。

- N 元模型广泛用于各种自然语言处理问题。通过 N 元模型，我们可以计算一个序列的概率，从而判断该序列是否符合自然语言的语法和语义规则。

**平滑技术**

- 数据稀疏问题：主要是由于训练样本不足而导致密度估计不准确。
- Zipf 定律：很难通过增加数据集来避免稀疏问题。
- 平滑技术：解决数据稀疏问题，即给一些没有出现的词组合赋予一定的先验概率。加法平滑的计算公式为：

$$
p(x_t \mid \mathbf{x}_{(t - N + 1):(t - 1)}) = 
\frac{\mathrm{m}(\mathbf{x}_{(t - N + 1):t}) + \delta}
     {\mathrm{m}(\mathbf{x}_{(t - N + 1):(t - 1)}) + \delta |\mathcal{V}|},

$$

其中 $\delta \in (0,1]$ 为常数，当 $\delta=1$ 时，称为加 1 平滑。

## 深度序列模型

深度序列模型是指利用神经网络模型来估计条件概率 $p_{\theta}(x_{t} \mid x_{1:(t-1)})$。

### 模型结构

深度序列模型一般可以分为三个模块：嵌入层；特征层；输出层。

#### 嵌入层

嵌入层：相当于一个嵌入表，通过嵌入表将每个需要输入的符号序列直接映射成向量表示：

$$
e_{t}=M \delta_{t}=m_{k}
$$

通过上面的映射可以得到序列对应的向量序列。

#### 特征层

特征层用于从输入向量序列中提取特征，输出一个可以表示历史信息的向量，其可以通过不同类型的神剧网络来实现，常见的网络类型有以下三种：

**简单平均**

历史信息的向量 $h_{t}$ 为前面 t-1 个词向量的平均，即

$$
h_{t}=\sum^{t-1}_{i=1}\alpha_{i} e_{i}
$$

权重 $\alpha_{i}$ 可以和位置 i 以及其表示 $e_{i}$ 相关，也可以无关。为了简单起见，可以设置 $\alpha_{i} = \frac{1}{t-1}$。权重 $\alpha_{i}$ 也可以通过注意力机制来动态计算。

**前馈神经网络**

前馈神剧网络要求输入的大小是固定的。因此，和 N 元模型类似，假设历史信息只包含前面 N-1 个词，首先将这 N-1 个词向量拼接成一个 $D_{x} \times (N-1)$ 维的向量，即：

$$
h' = e_{t-N+1} \oplus \cdots \oplus e_{t-1}
$$

然后将 $h'$ 输入到由前馈神经网络构成的隐藏层，最后一层隐藏层的输出 $h_{t}$，即

$$
h_{t} = g(h';\theta_{g})
$$

为了增加特征的多样性和提高模型训练效率，前馈神经网络中也可以包含跳层连接。

**循环神经网络**

循环神经网络可以接受长的输入序列，与前馈神经网络不同之处在于，循环神经网络利用隐藏状态来记录以前所有时刻的信息，而前馈神经网络只能接受 N-1 个时刻的信息。

#### 输出层

输出层一般使用 Softmax 分类器，接受历史信息的向量表示，输出为此表中每个词的后验概率，输出大小为 $|V|$

$$
\begin{align*}
o_t &= \mathrm{softmax}(\hat{o}_t) \\
&= \mathrm{softmax}(W h_t + b)
\end{align*}
$$

两种不同的深度序列模型如下图所示：

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250605111624903.png)

### 参数学习

给定一个训练序列，深度序列模型的训练目标是找到一组参数 $\theta$ 使得对数似然函数最大：

$$
\log p_\theta(x_{1:T}) = \sum_{t=1}^T \log p_\theta(x_t \mid x_{1:(t-1)})
$$

其中 $\theta$ 表示网络中的所有参数，包括嵌入矩阵 M 以及神经网络的权重以及偏置。

网络参数一般通过梯度上升法来进行学习，其中 $\alpha$ 为学习率：

$$
\theta \leftarrow \theta + \alpha \frac{\partial \log p_{\theta}(x_{1:T})}{\partial \theta}
$$

## 评价方法

### 困惑度

- 困惑度可以用来衡量一个分布的不确定性，对于离散随机便利 $X \in \mathcal{X}$，其概率分布为 $p(x)$，困惑度为:

$$
2^{H(p)}=2^{-\sum_{x\in X}p(x)\log_2p(x)}
$$

- 困惑度也可以用来衡量两个分布之间的差异：

$$
2^{H(\tilde{p}_{r},p_{\theta})}=2^{-\frac{1}{N}\sum_{n=1}^{N}\log_{2}p_{\theta}(x^{(n)})}
$$

- 困惑度还可以衡量模型分布与样本经验之间的契合程度，即模型分布的好坏，困惑度越低则两个分布越接近。

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250605114825803.png)

### BLEU 算法

BLEU 算法是一种衡量模型生成序列和参考序列之间 N 元词组重合度的算法，最早用来评价机器翻译模型的质量。

令 x 为从模型分布中生成的一个候选序列，$s^{(1)},\dots,s^{(K)}$ 为从真实数据分布中采集的一组参考序列，$\mathcal{W}$ 是从生成的候选序列中提取的所有 N 元组合的集合，这些 N 元组合的精度：

$$
P_{N}(x)=\frac{\sum_{w\in W}\min(c_{w}(x),\max_{k=1}^{K}c_{w}(s^{(k)}))}{\sum_{w\in W}c_{w}(x)}
$$

上式代表了生成序列中的 N 元组合有多少比例在参考序列中出现。

由于精度只衡量生成序列中的 N 元组合是否在参考序列中出现，生成序列越短，其精度会越高，因此可以引入长度惩罚因子，如果生成序列的长度短于参考序列，就对其进行惩罚：

$$
b(\boldsymbol{x})=\left\{
\begin{array}
{ccc}1 & \mathrm{if} & l_x>l_s \\
 \\
\exp\left(1-l_s/l_x\right) & \mathrm{if} & l_x\leq l_s
\end{array}\right.
$$

BLUE 算法是通过计算不同长度的 N 元组合的进度，并进行几何加权平均而得到：

$$\mathrm{BLEU-N}(\boldsymbol{x})=b(\boldsymbol{x})\times\exp\left(\sum_{N=1}^{N^{\prime}}\alpha_N\log P_N\right)
$$

BLEU 算法的值域范围为 $[0,1]$，越大表明生成的质量越好，但是 BLEU 算法只计算进度而不关心召回率。

### ROUGE 算法

ROUGE 算法与 BLEU 算法类似，但是 ROUGE 算法计算的是召回率：

$$
\mathrm{ROUGE-N}(x)=\frac{\sum_{k=1}^K\sum_{w\in W}\min\left(c_w(x),c_w(s^{(k)})\right)}{\sum_{k=1}^K\sum_{w\in W}c_w(s^{(k)})}
$$

## 序列生成模型中的学习问题

### 曝光偏差问题

- 教师强制学习方式：在自回归生成模型中，第 t 步的输入为模型生成的前缀序列 $\hat{x}_{1:(t-1)}$，而在训练时使用的前缀序列是训练集中的真实神剧 $x_{1:(t-1)}$，而不是模型预测的前缀序列。

这种教师强制的学习方式存在**协变量偏移问题**，一旦在预测前缀的过程中存在错误，会导致错误传播，使得后续生成的序列也会偏离真实分布，这个问题称为曝光偏差问题。

- 解决方法：使用**计划采样**，并且在训练时混合使用真实数据和模型生成数据，使用一个超参数 $\epsilon$ 为控制替换率的超参数，在一开始，将 $\epsilon$ 设为一个较大的值，然后随着训练次数增加逐步减小其取值。

在计划采样中可以通过下面几种方式来逐步减少 $\epsilon$ 的取值：

- 线性衰减: $\epsilon_{i} = \max(\epsilon, k - ci)$, 其中 $\epsilon$ 为最小的替换率, $k$ 和 $c$ 分别为初始值和衰减率.
- 指数衰减: $\epsilon_{i} = k^{i}$, 其中 $k < 1$ 为初始替换率.
- 逆 Sigmoid 衰减: $\epsilon_{i} = \frac{k}{k + \exp(i/k)}$, 其中 $k \geq 1$ 来控制衰减速度.

- 缺点：过度纠正，即每一步中不管如何选择，目标输出依然来自于真实数据。

### 训练不一致问题

- 主要指的是训练目标和评价方法不一致的问题。
- 解决方法：基于强化学习的序列生成，为了可以直接优化评价目标，我们可以将自回归序列生成看作一种马尔可夫决策过程，并使用强化学习的方法来进行训练。

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250605145820860.png)

这样，序列生成问题就转换成强化学习问题。同时，基于强化学习的序列生成模型不但可以解决训练和评价目标不一致问题，也可以有效解决曝光偏差问题。

### 计算效率问题

序列生成模型的输出层为词表中所有词的概率条件，需要 Softmax 归一化，当词表较大时，计算效率比较低。

在第t步时, 前缀序列为 $\tilde{h}_{t}=\boldsymbol{x}_{1:(t-1)}$, 词 $x_{t}$ 的条件概率为

$$
\begin{align*}
p_{\theta}(x_{t}|\tilde{h}_{t}) &=\operatorname{softmax}\left(s(x_{t},\tilde{h}_{t};\theta)\right) \\
&=\frac{\exp\left(s(x_{t},\tilde{h}_{t};\theta)\right)}{\sum_{v\in\mathcal{V}}\exp\left(s(v,\tilde{h}_{t};\theta)\right)} \\
&=\frac{\exp\left(s(x_{t},\tilde{h}_{t};\theta)\right)}{Z(\tilde{h}_{t};\theta)}
\end{align*}
$$

其中$s(x_{t},\tilde{h}_{t};\theta)$为未经过Softmax归一化的得分函数,$Z(\tilde{h}_{t};\theta)$为配分函数(Partition Function).
$$Z(\tilde{h}_{t};\theta)=\sum_{v\in\mathcal{V}}\exp\left(s(v,\tilde{h}_{t};\theta)\right).$$

每个样本都需要计算一次配分函数，使得整个训练过程十分消耗时间。因此我们可以使用一些近似估计的方法来加快训练速度：

1. 层次化 Softmax 方法，将标准 Softmax 函数的扁平结构转换为层次化结构
2. 基于采样的方法，通过采样来近似计算更新梯度

> [!note]
> 这一部分建议直接看书 P378，书上的公式和推导写的够详细了（虽然有些还是看不懂~~笑~~）：
> - 层次化 softmax
> - 重要性采样
> - 噪声估计对比

## 序列到序列模型

序列到序列 (Seq2Seq) 是一种条件的序列生成问题，即给定一个序列，生成另一个序列，输入序列的长度和输出序列的长度可以不同。例如翻译：

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250605152819256.png)

Seq2Seq 序列模型的目标是估计条件概率：

$$
p_\theta(y_{1:T}|x_{1:S})=\prod_{t=1}^Tp_\theta(y_t|y_{1:(t-1)},x_{1:S})
$$

给定一组训练数据，我们可以使用最大死然估计来训练模型参数：

$$
\hat{\theta}=\arg\max_\theta\sum_{n=1}^N\log p_\theta(\boldsymbol{y}_{1:T_n}|\boldsymbol{x}_{1:S_n})
$$

一旦训练完成，模型就可以根据一个输入序列 x 来生成最有可能的目标序列：

$$
\hat{\mathbf{y}}=\arg\max_yp_{\hat{\theta}}(y|x)
$$

条件概率可以通过不同的神经网络来实现。

### 基于 RNN 的 Seq2Seq 模型

- 也被称为编码器，解码器模型

**编码器**

首先使用一个 RNN 来编码输入序列的到一个固定维度的向量 u，u 一般为编码循环神剧网络最后时刻的隐状态。

$$\boldsymbol{h}_t^\mathrm{enc}=f_\mathrm{enc}(\boldsymbol{h}_{t-1}^\mathrm{enc},\boldsymbol{e}_{x_{t-1}},\theta_\mathrm{enc}),\quad\forall t\in[1:S]$$
$$
\boldsymbol{u}=\boldsymbol{h}^{enc}_{S}
$$

**解码器**

在生成目标序列时，使用另外一个 RNN 来进行解码。

$$\begin{aligned}
\boldsymbol{h}_0^{\mathrm{dec}} & =\boldsymbol{u}, \\
\boldsymbol{h}_t^{\mathrm{dec}} & =f_{\mathrm{dec}}(\boldsymbol{h}_{t-1}^\mathrm{dec},\boldsymbol{e}_{y_{t-1}},\theta_\mathrm{dec}), \\
\mathbf{0}_{t} & =g(h_t^{\mathrm{dec}},\theta_o),
\end{aligned}$$

基于 RNN 的 Seq2Seq 模型缺点如下：

1. 编码向量 u 容量问题
2. 长程依赖问题
3. 无法并行计算

### 基于注意力的 Seq2Seq 模型

为了获取更加丰富的输入序列信息，我们可以在每一步中通过注意力机制来从输入序列中获取有用的信息。

在解码过程的第 t 步时，先用上一步的隐状态 u $h^{dec}_{t-1}$ 作为查询向量，利用注意力机制从所有输入序列的隐状态 $H^{enc}=[h^{enc}_{1},\dots,h^{enc}_{S}]$ 中选择相关信息：

$$\begin{aligned}
\boldsymbol{c}_{t} & =\operatorname{att}(\boldsymbol{H}^\mathrm{enc},\boldsymbol{h}_{t-1}^\mathrm{dec})=\sum_{i=1}^S\alpha_i\boldsymbol{h}_i^\mathrm{enc} \\
 & =\sum_{i=1}^S\mathrm{softmax}\left(s(\boldsymbol{h}_i^\mathrm{enc},\boldsymbol{h}_{t-1}^\mathrm{dec})\right)\boldsymbol{h}_i^\mathrm{enc}
\end{aligned}$$

然后，从输入序列中训责的信息 $c_{t}$ 也作为解码器在第 t 步时的输入，得到第 t 步的隐状态：

$$\boldsymbol{h}_t^{\mathrm{dec}}=f_{\mathrm{dec}}(\boldsymbol{h}_{t-1}^{\mathrm{dec}},[\boldsymbol{e}_{y_{t-1}};\boldsymbol{c}_t],\theta_{\mathrm{dec}}).$$

最后，将 $h^{dec}_{t}$ 输入到分类器中来预测此表中每个词出现的频率。

### 基于自注意力的 Seq2Seq 模型

#### 自注意力

对于一个向量序列 $H=[h_{1},\dots,h_{T}] \in \mathbb{R}^{D_{h} \times T}$，首先使用自注意力模型来对其进行编码，即：

$$\begin{aligned}
 & \operatorname{self-att}(Q,K,V)=V\operatorname{softmax}\left(\frac{K^\intercal Q}{\sqrt{D_k}}\right), \\
 & Q=W_qH,K=W_kH,V=W_\upsilon H,
\end{aligned}$$
#### 多头注意力

自注意力模型可以看作在一个线性空间中建立 H 个不同向量之间的交互关系，为了提取更多的交互信息，我们可以使用多头自注意力，在多个不同的个投影空间中捕捉不同的交互信息，假设在 M 个投影空间中分别应用自注意力模型，则有：

$$
\begin{aligned}
 & \mathrm{MultiHead}(H)=W_{0}[\mathrm{head}_{1};\cdots;\mathrm{head}_{M}], \\
 & \mathrm{head}_{m}=\mathrm{self-att}(Q_{m},K_{m},V_{m}), \\
 & \forall m\in\{1,\cdots,M\},\quad Q_{m}=W_{q}^{m}H,K=W_{k}^{m}H,V=W_{v}^{m}H,
\end{aligned}
$$

#### 基于自注意力模型的序列编码

我们可以在初始的输入序列中加入位置编码：

$$p_{t,2i} = \sin(t/10000^{2i}/D) , p_{t,2i+1} = \cos(t/10000^{2i}/D)$$

其中 $p_{t, 2i}$ 表示第 t 个位置的编码向量的第 2i 维，D 是向量编码的维度。之后，计算 $H^{(l)}$ 层的隐状态，可以通过一个多头注意力模块以及一个非线性的前馈网络得到。每次计算都需要残差连接和层归一化操作：

$$\begin{aligned}
 & Z^{(l)}=\mathrm{norm}\left(\boldsymbol{H}^{(l-1)}+\mathrm{MultiHead}\left(\boldsymbol{H}^{(l-1)}\right)\right), \\
 & \boldsymbol{H}^{(l)}=\mathrm{norm}\left(\boldsymbol{Z}^{(l)}+\mathrm{FFN}(\boldsymbol{Z}^{(l)})\right),
\end{aligned}$$

$\text{norm}(\cdot)$ 表示层归一化，FNN 表示逐位置的前馈神经网络。

$$
\mathrm{FFN}(z)=W_2\mathrm{ReLu}(W_1z+b_1)+b_2
$$

基于自注意力模型的序列编码可以看作一个全连接的前馈神经网络，第 l 层的每个位置都接受第 l-1 层的所有位置的输出。不同的是，其连接权重是通过注意力机制动态计算得到。

#### Transformer 模型

Transformer 模型是一个基于多头注意力的 Seq2Seq 模型，其网络结构可以分为两个部分：

**编码器**

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250605163324123.png)

**解码器**

解码器是通过自回归的方式来生成目标序列，由以下三个模块组成：

- 掩蔽自注意力模块：第 t 步时，先使用自注意力模型对已生成的前缀序列进行编码得到 H
- 解码器到编码器注意力模块：将 $h^{dec}_{t}$ 进行线性映射得到 $q^{dec}_{t}$，然后将其作为查询向量，通过键值对注意力机制来从输入中获取有用的信息
- 逐位置的前馈神经网络：使用一个前馈神经网络来综合得到所有信息

将上述三个步骤重复多次，最后通过一个全连接前馈神经网络来计算输出概率。

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250605163803228.png)

