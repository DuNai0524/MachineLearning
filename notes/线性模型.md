---
tags:
---
线性模型指通过样本特征的线性组合来进行预测的模型。

- 线性组合函数：$f(x;\omega) = \omega^Rx + b$
- 决策函数：在分类问题中，由于输出目标 $y$ 是一些离散的标签，而 $f(x;\omega)$ 的值域为实数，因此需要引入一个非线性的**决策函数** $g(\cdot)$ 来预测输出目标，$f(x;w)$ 也叫作**判别函数**：$y=g(f(x;\omega))$
- 符号函数：$$
g(f(x;w)) = \begin{cases}
0 \ if \ f(x;w) > 0 \\ 1 \ if \ f(x;w) < 0
\end{cases}
=sgn(f(x;w))
$$

## 线性判别函数和决策边界

### 二分类

判别函数：$f(x;w)=w^Tx + b$

- 决策平面/决策边界：特征空间 $\mathbb{R}^D$ 总所有满足 $f(x;w)=0$ 的组合
- 有向距离：$\gamma=\frac{f(x;w)}{||w||}$
- 优化方法：0-1 损失函数，但数学性质不太好

>[!note] 定义 3.1 - 两类线性可分
> 对于训练集 $\mathcal{D}=\{(x^{(n)},y^{(n)})\}^N_{n=1}$，如果存在权重向量 $w^*$，对所有样本都满足 $yf(x;w^*)>0$，那么训练集 $\mathcal{D}$ 就是线性可分的

### 多分类

**设计判别函数的方式**

- "一对其余"方式：把多分类问题转换成 $C$ 个"一对其余"的二分类问题，需要 C 个判别函数
- "一对一"方式：把多分类问题转换成 $\frac{C(C-1)}{2}$ 个"一对一"二分类问题，需要 $\frac{C(C-1)}{2}$ 个函数，其中 (i, j) 是指将类别 i 与类别 j 的样本区分开
- "argmax"方式：改进的"一对其余"方式，需要 C 个判别函数，其中 $\underset{c=1}{\overset{C}{argmax}}f_{c(x;w_{c})}$，解决了其余两种方式难以确定类别的区域的问题

![Pasted image 20250509220337|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/Pasted%20image%2020250509220337.png?imageSlim)

> [!note] 定义 3.2 - 多类线性可分
> 对于训练集 $\mathcal{D} = \{(x^{(n)},y^{(n)})\}^N_{n=1}$，如果存在 C 个权重向量 $w^*_{1},\dots,w^*_{C}$，使得第 c ($1\le c \le C$) 类的所有样本都满足 $f_{c(x;w^*_{c}) > f_{\tilde{c}}(x;w^*_{\tilde{c}})} \ne c$，那么训练集 $\mathcal{D}$ 是线性可分的

## Logistic 回归

- 激活函数：也就是决策函数为 Logistic 函数
- $p(y=1|x) \stackrel{\triangle}{=} \frac{1}{1 + \exp(-w^T x)}$
- $p(y=0|x)=1-p(y=1|x)=\frac{\exp(-w^Tx)}{1+\exp(-w^Tx)}$
- 将第一个公式进行变换，可得：$w^Tx=\log \frac{p(y=1|x)}{p(y=0|x)}$，因此 Logistic 函数也被称为对数几率回归

### 参数学习

**交叉熵损失函数**

![Pasted image 20250511120645|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/Pasted%20image%2020250511120645.png?imageSlim)

**梯度下降法更新**

![Pasted image 20250511120722|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/Pasted%20image%2020250511120722.png?imageSlim)

- 风险函数 $\mathcal{R}(w)$ 是关于参数 $w$ 的连续可到的凸函数，因此除了梯度下降法之外，Logistic 回归还可以用高阶的优化方法 (比如牛顿法) 来进行优化.

## Softmax 回归

Softmax 回归相当于 Logistic 回归在多分类问题上的推广.

- Softmax 回归预测条件概率：$p(y=c|x)=\frac{\exp(w^T_{c}x)}{\sum^C_{c`=1}\exp(w^T_{c`}x)}$
- 决策函数：$\hat{y}=\underset{c=1}{\overset{C}{argmax}} \ w^T_{c}x$
- **与 Logistic 回归的关系**：当类别数 C=2 时，Softmax 回归的决策函数为 $\hat{y}=I((w_{1}-w_{0})^Tx>0)$
- 向量表示：$\hat{y}=\frac{\exp(W^Tx)}{1^T_{C\exp(W^Tx)}}$

### 参数学习

- 交叉熵损失函数：$\mathcal{R}(W)=-\frac{1}{N}\sum^N_{n=1}(y^{(n)})^T\log \hat{y}^{(n)}$
- 风险函数 $\mathcal{R}(W)$ 关于 $W$ 的梯度：$\frac{\partial \mathcal{R}(W)}{\partial W}=-\frac{1}{N}\sum^N_{n=1}x^{(n)}(y^{(n)}-\hat{y}^{(n)})^T$
- 证明：

![Pasted image 20250511160938|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/Pasted%20image%2020250511160938.png?imageSlim)

- 梯度更新：![Pasted image 20250511161705|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/Pasted%20image%2020250511161705.png?imageSlim)

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250512223910336.png?imageSlim)

## 感知器

感知器是最简单的人工神经网络，只有一个神经元。感知器是对生物神经元的简单数学模拟，有与生物神经元相对于的部件，输出为 +1 或者 -1.

$$
\hat{y}=sgn(w^Tx)
$$

### 参数学习

给定 N 个样本的训练集，感知器学习算法试图找到一组参数 $w^*$，使得对于每个样本，有:

$$
y^{(n)}w^{*T}x^{(n)} > 0, \ \ \ \ \ \  \forall n \in \{1,\dots,N\}
$$

感知器的学习算法是一种**错误驱动的在线学习算法**，每次分错一个样本的时候就用这个样本来更新权重。

- 损失函数：$\mathcal{L}(w;x,y)=\max(0,-yw^Tx)$
- 随机梯度下降：

$$
\frac{\partial \mathcal{L}(w;x,y)}{\partial w} = \begin{cases}
0 \ \ \ \ \ if \ \ yw^Tx > 0. \\
-yx \ \ \ \ \ if \ \ yw^Tx < 0
\end{cases}
$$

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250512223936432.png?imageSlim)


更新过程图示：

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250512223947497.png?imageSlim)

### 感知器的收敛性

证明对于两类问题，如果训练集是线性可分的，那么感知器算法可以在有限次迭代后收敛. 然而，如果训练集不是线性可分的，那么这个算法则不能确保会收敛。

若数据集两类线性可分，对于训练集，那么存在一个正的常数 $\gamma \ (\gamma > 0)$ 和权重向量 $w^*$，并且 $||w^*||=1$，则对所有 n 满足 $(w^*)^T(y^{(n)}x^{(n)}) \ge \gamma$，我们可以证明以下定理：

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250512223959581.png?imageSlim)

证明过程：

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250512224027523.png?imageSlim)

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250512224037975.png?imageSlim)

感知器的不足：

- 在数据集线性可分时，感知器虽然可以找到一个超平面将两类数据分开，但并不能保证其泛化能力.
- 感知器对于样本顺序比较敏感，每次迭代的顺序不一样的时候，找到的分割平面也往往不一样
- 如果训练集不是线性可分的，那么永远都不会收敛

### 参数平均感知器

感知器有时候并不能保证找到的判别函数是最优的，这样可能导致过拟合。

因此，为了提高感知器的鲁棒性和泛化能力，我们可以将在感知器学习过程中的所有 K 个权重向量保存起来，并赋予每个权重向量 $w_{k}$ 一个置信系数 $c_{k} \ \ (1 \leq k \leq K)$，最终的分类结果通过这 K 个不同权重的感知器投票决定，这个模型也称为**投票感知器**. 形式如下：

$$
\hat{y}=sgn\left( \sum^K_{k=1} c_{k} sgn(w^T_{k}x) \right)
$$

然而，保存 K 个权重向量在实际操作中会带来额外的开销，因此我们使用**平均感知器**来减少投票感知器的参数数量. 形式如下：

$$
\hat{y}=sgn(\overline{w}^Tx)
$$

改进的平均感知器算法的训练过程：

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250512224059202.png?imageSlim)

### 扩展到多分类

原始的感知器可以扩展到多分类问题，甚至是更一般的**结构化学习问题**.

引入一个构建在输入输出联合空间上的特征函数 $\phi(x,y)$，将样本对 $(x,y)$ 映射到一个特征向量空间，可以构建一个广义的感知器模型：

$$
\hat{y}= \underset{y \in Gen(x)}{\arg \max w^T \phi(x,y)}
$$

- 一种常用的特征函数：

$$
\phi(x,y) = vec(xy^T) \in \mathbb{R}^{(D \times C)}
$$

**广义感知器参数学习方法**

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250512224113965.png?imageSlim)

**广义感知器的收敛性**

广义线性可分条件定义：

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250512224420100.png?imageSlim)

广义感知器收敛性定义如下：

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250512224428594.png?imageSlim)

## 支持向量机

支持向量机 (SVM) 是一个经典的二分类算法，其找到了分割超平面具有更好的鲁棒性，因此广泛是用在很多任务上，并表现出了很强的优势。

- 决策函数：$f(x) = sgn((w^*)^T + b^*)$
- 目标优化函数：

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250512224438497.png?imageSlim)

数据中所有满足 $y^{(n)}(w^Tx^{(n)} + b) = 1$ 的样本点都称为**支持向量**：

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250512224450029.png?imageSlim)

### 参数学习

为了找到最大间隔分割超平面，可以将目标函数写为凸优化问题，并使用拉格朗日数乘法：

$$
\Lambda(w,b,\lambda)=\frac{1}{2}||w||^2 + \sum^N_{n=1}\lambda_{n}(1-y^{(n)}(w^Tx^{(n)} + b))
$$

计算上式关于 $w$ 和 $b$ 的导数，进而可以得到拉格朗日对偶函数：

$$
\Gamma(\lambda)=-\frac{1}{2}\sum^N_{n=1}\sum^N_{m=1}y^{(m)}y^{(n)}(x^{(m)})^Tx^{(n)} + \sum^N_{n=1} \lambda_{n}
$$

- 优化方法：可以通过多种凸优化方式进行优化，但是由于约束条件为训练样本数量，一般的优化方法代价比较高，因此在实践中常常使用 SMO 算法 (序列最小优化算法)，可以获得全局最优解。

### 核函数

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250512224500524.png?imageSlim)

使用核函数，隐式地将样本从原始特征空间映射到更高维的空间，并解决原始特征空间中的线性不可分问题。

### 软间隔

避免在特征空间中出现线性不可分的情况，引入一个松弛变量 $\xi$：

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250512224507829.png?imageSlim)

也可以表示为经验风险 + 正则化项的形式：

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250512224519003.png?imageSlim)

## 损失函数对比

- Logistic 回归的损失函数

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250512224527261.png?imageSlim)

- 感知器的损失函数

$$
\mathcal{L}_{p}=\max(0, -yf(x;w))
$$

- 软间隔支持向量机的损失函数

$$
\mathcal{L}_{hinge}=\max(0, 1-yf(x;w))
$$

- 平方损失可以重写为

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250512224535133.png?imageSlim)

- 不同损失函数的对比：

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250512224546829.png?imageSlim)

可以看出来，除了平方损失函数，其他损失函数都比较适合于二分类问题.

## 总结

几种常见的线性模型对比：

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250512224555803.png?imageSlim)
