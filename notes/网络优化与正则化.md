---
tags:
  - 深度学习
---
神经网络的种类非常多，由于网络结构的多样性，我们很难找到一种通用的优化方法，不同优化方法在不同网络结构上的表现也有比较大的差异。此外，网络的超参数一般比较多，这也给优化带来很大的挑战。

## 网络优化

网络优化指的是寻找一个神经网络模型来使得经验（或结构）风险最小化的过程。此外，深度神经网络还存在梯度消失问题。

### 高维变量非凸优化

高维变量优化与相对低维的变量优化的挑战以及问题有所不同。

**鞍点**

在高维空间中，非凸优化的难点并不在如何逃离局部最优点，还是如何逃离**鞍点**。因为鞍点在有些维度上是最高点，有些维度上是最低点。

基于梯度下降的优化方法会在鞍点附近接近于停滞，通过在梯度方向上引入随机性，可以有效的逃离鞍点。

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250521205732845.png?imageSlim)


**平坦最小值**

深度神经网络参数非常多，并且有一定的冗余性，使得每个参数对最终损失的影响都会比较小，因此会导致损失函数在具备最小值附近通常是一个平坦的区域，叫作平坦最小值。当一个模型收敛到一个平坦的局部最小值时，鲁棒性会更好。

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250521205759024.png?imageSlim)

**局部最小解等价性**

在非常大的神经网络中，大部分的局部最小解是等价的，在测试集上性能比较相似。

### 神经网络优化的改善方法

1. 使用更有效的优化算法来提高梯度下降方法和稳定性
2. 使用更好的参数初始化方法、数据预处理方法
3. 修改网络结构来得到更好的优化地形
4. 使用更好的超参数优化方法

## 优化算法

### 小批量梯度下降

令 $f(x;\theta)$ 表示一个深度神经网络，$\theta$ 为网络参数，每次选取 K 个训练样本，在使用小批量梯度下降进行优化书，第 t 次迭代时损失函数关于参数 $\theta$ 的偏导数为：

$$
\mathcal{g}_{t}(\theta) = \frac{1}{K}\sum_{(x, y) \in S_{t}} \frac{\partial \mathcal{L}(y, f(x;\theta))}{\partial \theta}
$$

其中 $\mathcal{L}(\cdot)$ 是可微分的损失函数，K 被称为批量大小。

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250522110835375.png?imageSlim)

影响小批量梯度下降主要因素：

1. 批量大小 K
2. 学习率 $\alpha$
3. 梯度估计

### 批量大小选择

- 线性缩放规则：当批量增加大小 m 倍时，学习率也增加 m 倍. 在批量大小比较少时使用，当批量大小非常大时会使得训练不稳定。此外，批量大小和模型的泛化能力也有一定的关系：批量越大，越有可能收敛到尖锐最小值；批量越小，越有可能收敛到平坦最小值。

$$
1 \ \text{回合}(Epoch) = (\frac{训练样本的数量K}{批量大小K}) \times 迭代 \ (Iteration)
$$

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250522152457892.png?imageSlim)

### 学习率调整

学习率是神经网络优化时的重要超参数。在梯度下降法中，学习率 $\alpha$ 的取值非常关键，如果过大就不会收敛，过小就会收敛太慢。因此需要对每个参数设置不同的学习率。

#### 学习率衰减

学习率在一开始一般要保持大一些来保证收敛速度，在收敛到最优点附近时要小一些来避免来回振荡。比较简单的调整可以通过**学习率衰减**的方式来实现，也称为**学习率退火**。

假设初始化学习率为 $\alpha_{0}$，在第 t 次迭代式的学习率为 $\alpha_{t}$，常学习率退火见学习率衰减方法如下($\beta$ 为衰减率，$T$ 为总的迭代次数)：

**分段函数衰减**

每经过 $T_{1},T_{2},\dots,T_{m}$ 次迭代将学习率减为原来的 $\beta_{1},\beta_{2},\dots,\beta_{m}$ 倍，其中 $T_{m}$ 和 $\beta_{m} < 1$ 为根据经验设置的超参数，分段函数衰减也称为**阶梯衰减**。

**逆时衰减**

$$
\alpha_{t} = \alpha_{0} \frac{1}{1 + \beta \times t}
$$

**指数衰减**

$$
\alpha_{t} = \alpha_{0} \beta^t
$$

**自然指数衰减**

$$
\alpha_{t} = \alpha_{0} \exp(-\beta \times t)
$$

**余弦衰减**

$$
\alpha_{t} = \frac{1}{2} \alpha_{0}(1+\cos(\frac{t \pi}{T}))
$$

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250522155022903.png?imageSlim)

#### 学习率预热

为了提高训练的稳定性，可以在最初几轮迭代时，采用比较小的学习率，等梯度下降到一定程度后再恢复到初始的学习率，即为学习率预热。

一个常用的学习率预热方法是逐渐预热。假设预热的迭代次数为 $T'$，初始学习率为 $\alpha_{0}$，在预热过程中，每次更新的学习率为:

$$
\alpha'_{t}=\frac{t}{T'}\alpha_{0}
$$

当预热过程结束，再选择一种学习率衰减方法来主键降低学习率。

#### 学习率周期性调整

为了使得梯度下降法能够逃离鞍点或者尖锐最小值，一种实验性的方法是在训练过程中周期性地增大学习率。周期性地增大学习率虽然可能短期内损害优化过程，使得网络收敛的稳定性变差，但从长期来看有利于找到更好的局部最优解。

**循环学习率**

循环学习率，让学习率在一个区间内周期性地增大和缩小，通常可以使用线性缩放来调整学习率，称为三角循环学习率。假设每个循环的长度都等于 $2\Delta T$，其中前 $\Delta T$ 步为学习率增大阶段，后 $\Delta T$ 为学习率缩小截断，在第 t 次迭代时，其所在的循环周期数 m 为：

$$
m=\left\lfloor  1 + \frac{t}{2 \Delta T}  \right\rfloor 
$$

第 t 次迭代的学习率为：

$$
\alpha_{t} = \alpha_{min}^m + (\alpha^m_{max} - \alpha^m_{min})(\max(-, 1 - b))
$$

$b \in [0,1]$ 的计算为：

$$
b = \mid \frac{t}{\Delta T} - 2m + 1 \mid
$$

**带热重启的随机梯度下降**

使用热重启方式来替代学习率衰减的方法，学习率每个一定周期之后重新初始化为某个余弦设定值，然后逐渐衰减。每次重启后模型不是从头开始优化，而是从重启前的参数基础上继续优化。

假设在梯度下降过程中重启 M 次，第 m 次重启在上次重启开始第 $T_{m}$ 个回合后进行，$T_{m}$ 称为重启周期。在第 $m$ 次重启之前，采用余弦耍奸来降低学习率，第 t 次迭代的学习率为：

$$
\alpha_{t} = \alpha^m_{min} + \frac{1}{2}(\alpha^m_{max} - \alpha^m_{min})\left( 1 + \cos\left( \frac{T_{cur}}{T_{m}} \right)  \pi \right)
$$



![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250522163034109.png?imageSlim)

#### AdaGrad 算法

借鉴 $l_{2}$ 正则化的思想，每次迭代的时候自适应地调整每个参数的学习率，在第 $t$ 次迭代式，先计算每个参数梯度平方的累计值：

$$
G_{t} = \sum^t_{\tau=1} g_{\tau} \odot g_{\tau}
$$

AdaGrad 算法的参数更新差值为：

$$
\Delta \theta_{t} = - \frac{\alpha}{\sqrt{ G_{t} + \epsilon }} \odot g_{t}
$$

- 缺点：经过一定次数的迭代依然没有找到最优点时，由于这时候的学习率已经比较小，很难再继续寻找到最优点。

#### RMSprop 算法

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250522164654049.png?imageSlim)

- 与 AdaGrad 算法的区别：$G_{t}$ 的计算由累计方式变成了指数衰减移动平均。在迭代过程中，每个参数的学习率并不是呈衰减趣事，既可以增大也可以减小。

#### AdaDelta 算法

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250522165035858.png?imageSlim)

### 梯度估计修正

梯度估计修正是一种有效地缓解梯度估计随机性的方式，可以通过使用最近一段时间内的平均梯度来代替当前时刻的随机梯度来作为参数更新的方向，从而提高优化速度。

#### 动量法

使用之前累计的动量来代替真正的梯度：

$$
\Delta \theta_{t} = \rho \Delta \theta_{t-1} - \alpha g_{t} = - \alpha \sum^t_{\tau = 1} \rho^{t - \tau}g_{\tau} 
$$

> [!note]
> 某种角度来说，当期梯度叠加上部分的上次梯度，一定程度上可以近似看作二阶梯度

#### Nesterov 加速梯度

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250522170305283.png?imageSlim)

#### Adam 算法

- 动量法和 RMSprop 算法的结合
- 平常一直在用，学习率敏感

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250522170357503.png?imageSlim)

#### 梯度截断

当梯度的模大于一定阈值时，结对梯度进行截断。梯度截断是一种比较简单的启发式方法，把梯度的模限定在一个区间，当梯度的模小于或大于这个区间时就进行截断，一般截断的方式有以下几种：

**按值截断**

在第 t 次迭代时，梯度为 $g_{t}$，给定一个区间 $[a,b]$，如果一个参数的梯度小于 a 时，就将其设为 a ；如果大于 b 时，就设为 b

$$
g_{t} = \max(\min(g_{t}, b), a)
$$

**按模截断**

$||g_{t}||^2 \leq b$ 时，保持 $g_{t}$ 不变，否则进行截断操作：

$$
g_{t} = \frac{b}{||g_{t}||}g_{t}
$$

### 优化算法小结

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250522171218338.png?imageSlim)

## 参数初始化

参数初始化的方式一般有以下三种：

1. 预训练初始化

- 提供一个好的参数初始值
- 监督学习或无监督学习任务
- 预训练模型在目标人物上的学习过程也称为精调
- 有更好的收敛性和泛化性，但灵活性不够

2. 随机初始化

- 打破对称权重现象
- 使得不同神经元之间的区分性更好

3. 固定值初始化

- 根据经验使用一个特殊的固定值来进行初始化

常用随机初始化方法如下：

### 基于固定方差的参数初始化

- 高斯分布初始化
- 均匀分布初始化

> [!note]
> 需要配合**逐层归一化**来使用

### 基于方差缩放的参数初始化

根据神经元的连接数量来自适应调整初始化分布的方差，则为方差缩放。

#### Xavier 初始化

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250522173326855.png?imageSlim)
![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250522173346380.png?imageSlim)

#### He 初始化

- 第 l 层神经元使用 ReLU 激活函数
- 参数理想方差：$var(w_{i}^{(l)}) = \frac{2}{M_{l-1}}$ 
- 若使用均匀分布：$r=\sqrt{ \frac{6}{M_{l-1}} }$

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250522173723356.png?imageSlim)

### 正交初始化

- 由于是对权重矩阵中每个参数进行独立采样，由于采样的随机性，采样出来的权重矩阵可能依然存在梯度消失或梯度爆炸问题
- 为了避免梯度消失或梯度爆炸问题，希望误差项在反向传播中具有范数保持性：$||\delta^{(l-1)}||^2 = ||\delta^{(l)}||^2 =||(W^{(l)})^T \delta^{(l)}||^2$

## 数据预处理

- 归一化：泛指把数据特征转换为相同尺度的方法。
- 最小最大值归一化：$\hat{x}^{(n)}=\frac{x^{(n)} - \min_{n}(x^{(n)})}{\max_{n}(x^{(n)}) - \min_{n}(x^{(n)})}$
- 标准化：$\hat{x}^{(n)} = \frac{x^{(n)} - \mu}{\sigma}$
- 白化：降低数据特征之间的冗余性，一个主要实现方式为 PCA 方法：

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250522195525290.png?imageSlim)

## 逐层归一化

逐层归一化是将传统机器学习中的数据归一化方法应用到深度神经网络中，对神经网络中的隐藏层的输入进行归一化，从而使得网络更容易地进行训练。

逐层归一化提升训练效率是通过以下几个方面：

1. 更好的尺度不变性，缓解内部协变量偏移现象。
2. 更平滑的优化地形。

> [!note] 关于内部协变量偏移
> 如果一个神经层的输入分布发生了改变，那么其参数就需要重新学习

### 批量归一化

- 是一种有效的逐层归一化方法，可以对神经网络中任意地中间层进行归一化操作
- 为了提高优化效率，就需要使得净输入分布一致。在实践中归一化操作一般应用在仿射变换之后，激活函数之前。即 $a^{(l)}=f(z^{(l)})=f(Wa^{(l-1)} + b)$ 中 $Wa^{(l-1)}+b$ 之后，进入激活函数计算之前进行归一化。
- 为了提高归一化效率，一般使用标准化将净输入的每一维都归一到标准正态分布：

$$
\hat{z}^{(l)}=\frac{z^{(l)} - \mathbb{E}[z^{(l)}]}{\sqrt{ \\text{var}(z^{(l)}) + \epsilon}}
$$

其中 $z^{(l)}$ 的方差使用当前小批量样本集的均值和方差近似估计：

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250523154407944.png?imageSlim)

- 为了使得归一化不对网络的表示能力造成负面影响（例如：使用 Sigmoid 时，取值区间刚好是接近线性变换区间，减弱了神经网络非线性性质），可以通过一个附加的缩放和平移变换来改变取值区间：

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250523154459299.png?imageSlim)

- 从最保守的角度考虑，可以通过标准归一化的逆变换来使得归一化后的变量可以被还原成原来的值：当 $\gamma = \sqrt{ \sigma^2_{\mathcal{B}} },\beta=\mu_{\mathcal{B}}$ 时，$\hat{z}^{(l)}=z^{(l)}$ 
- 归一化操作可以看作一个特殊的神经层，加在每一层非线性激活函数之前，即：

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250523160253173.png?imageSlim)

- 逐层归一胡不但可以提高优化效率，还可以作为一种隐形的正则化方法。
- 可以提高网络的泛化能力

### 层归一化

- 对一个中间层所有神经元进行归一化

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250523161010521.png?imageSlim)

**RNN 中的层归一化**

在 RNN 中，循环神经层的净输入一般会随着时间慢慢变大或者变小，从而导致梯度爆炸或者梯度消失。而层归一化的神经网络可以有效地缓解这种情况。

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250523162706094.png?imageSlim)

### 权重归一化

对神经网络的连接权重进行归一化;

$$
W_{i,:}=\frac{g_{i}}{||v_{i}||}v_{i}
$$

由于神经网络中权重经常是共享的，权重数量往往比神经元数量少，因此权重归一化的开销会比较小。

### 局部响应归一化

- 是一种受生物学启发的方法，通常用在基于卷积的图像处理上。
- 对邻近的特征映射进行局部归一化；
- 与层归一化对比：都是对同层的神经元进行归一化，不同的是局部响应归一化应用在激活函数之后，只是对邻近的神经元进行局部归一化，且不减去均值；
- 与最大汇聚的对比：最大汇聚是对同一个特征映射中的邻近位置中的神经元进行抑制；局部归一化是对同一个位置的邻近特征映射中的神经元进行抑制。

## 超参数优化

常见的超参数：

1. 网络结构
2. 优化参数
3. 正则化系数

超参数优化主要存在的问题：

1. 超参数优化是一个组合优化问题，无法像一般参数那样通过梯度下降方法来优化，也没有一种通用的优化方法
2. 评估一组超参数配置的事件代价非常高，从而导致一些方法在超参数优化中难以应用

### 网格搜索

是一种通过尝试所有超参数的组合来寻址合适一组超参数配置的方法。

### 随机搜索

对于超参数进行随机组合，然后选取一个最好的配置。

### 贝叶斯优化

根据当前已经试验的超参数组合，来预测下一个可能带来更大收益的组合（自适应的超参数优化方法）；

- 常用的优化方法：时序模型优化

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250523170959684.png?imageSlim)

### 动态资源分配

- 关键：将有限的资源分配给更有可能带来收益的超参数组合
- 一种逐次减半的动态资源分配方法：

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250523171309408.png?imageSlim)

### 神经架构搜索

- 通过神经网络。自动实现网络架构的设计。
- 利用元学习的思想，神经网络搜索利用一个控制器来生成另一个子网络的架构描述。

## 网络正则化

是一类通过限制模型复杂度，从而避免过拟合，提高泛化能力的方法。

### l1 和 l2 正则化

- 是机器学习中最常用的正则化方法，通过约束参数的 l1 和 l2 范数来减小模型在训练数据集上的过拟合现象

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250523172811785.png?imageSlim)

### 权重衰减

在每次参数更新时，引入一个衰减系数：

$$
\theta_{t} \leftarrow (1-\beta)\theta_{t-1} - \alpha g_{t}
$$

- 在标准随机梯度下降中，权重衰减和 l2 效果相同，但是权重衰减正则化和 l2 并不等价

### 提前停止

当验证集上错误率不再下降，就停止迭代。

### 丢弃法 (Dropout)

- 训练一个深度神经网络时，可以通过丢弃一部分神经元来避免过拟合
- 保留率是一个更接近于 1 的数，使得输入变化不会太大
- 对输入层神经元进行丢弃时，相当于增加噪声，提高网络的鲁棒性
- 一般是针对神经元进行丢弃，也可一是针对神经元之间的连接进行丢弃

> [!note] RNN 上的 Dropout
> 在 RNN 上，不能直接对每个时刻的隐状态进行随机丢弃，会损害 RNN 在时间维度上的记忆能力。一种简单的方法是对非时间维度的连接进行随机丢失。

### 数据增强

- 在数据量有限的情况下，通过数据增强来增加数据量，提高模型鲁棒性，避免过拟合
- 主要方法：
	- 旋转
	- 翻转
	- 缩放
	- 平移
	- 加噪声

### 标签平滑

- 在输出标签中添加噪声来避免模型过拟合