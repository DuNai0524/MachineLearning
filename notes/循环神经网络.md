---
tags:
  - 深度学习
---
循环神经网络 (RNN) 是一类具有短期记忆能力的神经网络。在 RNN 中，神经元不但可以接收其他神经元的信息，也可以接受自身的信息，形成具有环路的网络结构。

RNN 也可以很容易地扩展到两种更加广义的记忆网络模型：递归神经网络和图网络。

## 给网络添加记忆能力

### 延时神经网络

建立一个额外的延时单元，用来存储网络的历史信息, 典型的即**延时神经网络(TDNN)**。TDNN 是在前馈网络中的非输出层都添加一个延时器，记录神经元的最近几次活性值。在第 t 个时刻，第 l 层神经元的活性值依赖于第 l-1 层的神经元的最近 K 个时刻的活性值，即：

$$
h_{t}^{(l)}=f(h_t^{(l-1)}, h_{t - 1}^{(l-1)},\dots,h_{t - K}^{(l-1)})
$$

### 有外部输入的非线性自回归模型

- 自回归模型：用一个变量 $y_{t}$ 的历史信息来预测自己。

$$
y_{t} = w_{0} + \sum^K_{k=1}w_{k}t_{t-k} + \epsilon
$$

- 有外部输入的非线性自回归模型（NARX）：是自回归模型的扩展，通过一个延时器记录最近 $K_{x}$ 次的外部输入和最近 $K_{y}$ 次的输出：

$$
y_{t} = f(x_{t}, x_{t-1},\dots,x_{t-K_{x}},y_{t-1}, y_{t-2},\dots,y_{t-K_{y}})
$$

### 循环神经网络

- 循环神经网络（RNN）：通过使用带自反馈的神经元，能够处理任意长度的时序数据。给定一个输入序列 $x_{1:T}$，通过共公式更新待反馈边的隐藏层的活性值 $h_{t}$：

$$
h_{t} = f(h_{t-1}, x_{t})
$$

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250516173021773.png?imageSlim)


$h_{t}$ 在很多数学文献上也称为**状态**或者**隐状态**。理论上，RNN 可以近似任意地非线性动力系统。FNN 可以模拟任何连续函数，RNN 可以模拟任何程序。

## 简单循环网络 SRN

- SRN 是只有一个隐藏层的神经网络。
- 在一个两层的 FNN 中，链接存在相邻的层与层之间，隐藏层的节点之间是无连接的。而 SRN 增加了从隐藏层到隐藏层的反馈连接。

更新公式如下：

$$
h_{t}=f(Uh_{t-1}+Wx_{t}+b)
$$

其中 $z_{t}$ 为隐藏层净输入，$U \in \mathbb{R}^{D \times D}$ 为状态权重矩阵，$W \in \mathbb{R}^{D \times M}$ 为状态-输入权重矩阵，$b \in \mathbb{R}^{D}$ 为偏置向量，$f(\cdot)$ 是非线性激活函数，通常为 Tanh 函数或者 Logistic 函数。

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250516221601525.png?imageSlim)

### RNN 的计算能力

定义一个完全链接的 RNN：

$$
h_{t}=f(Yh_{t-1} + Wx_{t} + b)
$$
$$
y_{t} = Vh_{t}
$$

**RNN 的通用近似定理**

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250516221944409.png?imageSlim)

**图灵完备**

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250516222014321.png?imageSlim)

## 应用到机器学习

RNN 可以应用到不同的机器学习任务，根据每个任务的特点可以分为以下几种模式：

### 序列到类方式

- 主要用于序列数据的问题：输入为序列，输出为类别。

$$
\hat{y} = g(h_{T})
$$

- $h_T$ 可以看作整个序列的最终表示；$g(\cdot)$ 可以使简单的线性分类器或者复杂的分类器。
- 除了将最后时刻的状态作为整个序列的表示之外，我们还可以对整个序列的所有状态进行平均，并用这个平均状态来作为整个序列的表示。

$$
\hat{y}=g\left( \frac{1}{T}\sum^T_{t=1} h_{t} \right)
$$

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250516230202161.png?imageSlim)

### 同步的序列到序列方式

- 主要用于序列标注任务，每一个时刻都有输入和输出，输入序列和输出序列的长度相同。
- 每个时刻的隐状态 $h_{t}$ 代表了当时时刻的和历史的信息，并输入给分类器 $g(\cdot)$ 得到当前时刻的标签为 $\hat{y}$。

$$
\hat{y}_{t}=g(h_{t}) \ \ \ \ \ \ \forall t \in [1, T]
$$

### 异步的序列到序列方式

- 也被称为编码器-解码器模型，即输入序列和输出序列不需要有严格的对应关系，也不需要保持相同的长度，例如机器学习中翻译问题。
- 先将样本按不同时刻输入到编码器，得到编码 $h_{T}$，然后再使用另一个解码器输出序列 $\hat{y}_{1:M}$
- 通常使用非线性的自回归模型

$$
\begin{align*}
h_{t} = f_{1}(h_{t-1}, x_{t}) \ \ \ \ \ \ \forall t \in [1,T] \\
h_{T+t} = f_{2}(h_{T+t-1}, \hat{y}_{t-1}) \ \ \ \  \forall t \in[1, M] \\
\hat{y} = g(h_{T+1}) \ \ \ \ \ \forall t \in [1,M]
\end{align*}
$$

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250517095750189.png?imageSlim)

## 参数学习

RNN 的参数可以通过梯度下降法来进行学习。
### 随时间反向传播算法

思想类似前馈神经网络的误差方向传播算法。将 RNN 看作一个展开的多层前馈网络，所有层的参数是共享的，因此参数的真实梯度是所有“展开层”的参数梯度之和。

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250517101315295.png?imageSlim)

- $z_{k}=Uh_{k-1} + Wx_{k} + b$
- 计算复杂度：参数的梯度需要在一个完整的“向前”计算和“反向”计算后才能进行参数更新。并且需要保存所有时刻的中间梯度，空间复杂度比较高。

### 实时循环学习算法

- 通过向前传播来计算梯度
- 计算复杂度：不需要梯度回传，适合用于需要在线学习或者无限序列的任务中。

## 长程依赖问题

RNN 在学习过程中主要问题是梯度消失或者梯度爆炸问题，RNN 难以建模这种长距离的依赖关系：

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250517102715198.png?imageSlim)

### 改进方案

**梯度爆炸**

一般通过权重衰减和梯度截断来避免。

- 权重衰减：增加 $l_1$ 和 $l_{2}$ 范数的正则化来限制参数的取值范围，从而使得 $\gamma \leq 1$。
- 梯度截断：当梯度的模大于一定阈值时，将他截断成一个较小的数。

**梯度消失**

- 改变模型
- 使得 $h_{t}$ 与 $h_{t-1}$ 之间既有线性关系，也有非线性关系：

$$
h_{t} = h_{t-1} + g(x_{t}, h_{t-1}; \theta)
$$

但是存在两个问题：

- 梯度爆炸问题
- 记忆容量问题

引入门控机制来进一步改变模型。

## 基于门控的循环神经网络

### 长短期记忆网络

- 关于内部状态：

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250517104937455.png?imageSlim)

- 使用了三个门：

- 遗忘门 $f_{t}$：控制上一个时刻的内部状态 $c_{t-1}$ 需要遗忘多少信息。
- 输入门 $i_{t}$：控制当前时刻的候选状态 $\tilde{c}_{t}$ 有多少信息需要保存。
- 输出门 $o_{t}$ 控制当前时刻的内部状态 $c_{t}$ 有多少信息需要输出给外部状态 $h_{t}$。

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250517105159608.png?imageSlim)

结构图如下：

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250517105221588.png?imageSlim)

公式简洁描述如下：

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250517105239448.png?imageSlim)

### LSTM 的各种变体

**无遗忘门的 LSTM 网络**

$$
c_{t} = c_{t-1} + i_{t} \odot \tilde{c}_{t} 
$$

记忆单元会不断增大，降低 LSTM 模型的性能。

**peephole 连接**

三种元素同时依赖：

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250517105710016.png?imageSlim)

**耦合输入门和遗忘门**

减少 LSTM 计算复杂度，将两个门合并：

$$
c_{t} = (i - i_{t}) \odot c_{t - 1} + i_{t} \odot \tilde{c}_{t}
$$

### 门控循环单元网络

GRU 网络引入一个更新门来控制当前状态需要从历史状态中保留多少信息，以及需要从候选状态中接受多少新信息。

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250517110425235.png?imageSlim)

- $g(x_{t}, h_{t-1}; \theta)$ 定义如下：

$$
\begin{gather*}
\tilde{h}_{t} = \tanh\left( W_{h}x_{t} + U_{h}(r_{t} \odot h_{t-1}) + b_{h} \right) \\ 
r_{t} = \sigma(W_{r}x_{t} + U_{r}h_{t - 1} + b_{r})
\end{gather*}
$$

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250517110912656.png?imageSlim)

## 深层循环神经网络

### 堆叠循环神经网络

- 也被称为循环多层感知器

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250517111009080.png?imageSlim)

### 双向循环神经网络

由两层 RNN 组成，输入相同，只不过是信息传递的方向不同：

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250517111110523.png?imageSlim)

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250517111124176.png?imageSlim)

## 扩展到图结构

### 递归神经网络

RNN 在有向无循环图上的扩展，一般为树状的层次结构：

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250517111223158.png?imageSlim)

![image.png|300](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250517111245947.png?imageSlim)

- 用途：建模自然语言句子的语义
- 使用门控机制来改进递归神经网络中长程依赖问题，比如树结构的长短期记忆模型。

### 图神经网络

- 将消息传递的思想扩展到图数据结构：

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250517111417957.png?imageSlim)

- 其中 $\mathcal{N}(v)$ 表示节点 $v$ 的邻居，$m^{(v)}_{t}$ 表示在第 t 时刻节点 v 收到的信息，$e^{(u, v)}$ 为边 $e^{(u, v)}$ 上的特征。
- 以上两个公式是一种同步的更新方式，所有的结构同时接受信息并更新自己的状态。而对于有向图来说，使用异步的更新方式 UIM 卡更有效率，比如循环神经网络或递归神经网络。
- 在整个图更新 T 次之后，可以通过一个读出函数来得到整个网络的表达：

$$
o_{t} = g(\{ h_{T}^{(v)} \mid v \in \mathcal{V} \})
$$