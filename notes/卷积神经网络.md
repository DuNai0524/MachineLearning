---
tags:
  - 深度学习
---
如果使用全连接前馈网络来处理图像，则会存在以下两个问题：

1. 参数太多
2. 局部不变性特征

卷积神经网络 (CNN) 是一种具有**局部连接、权重共享**等特性的深层前馈神经网络。目前的 CNN 一般是由卷积层、汇聚层和全连接层交叉堆叠而成的 FNN。CNN 有三个结构上的特性：**局部连接**、**权重共享**以及**汇聚**。

## 卷积

### 卷积的定义

#### 一维卷积

一维卷积常用于信号处理中，用于计算信号的延迟累计。

$$
y_{t} = \sum^{K}_{k=1} w_{k}x_{t-k+1}
$$

其中 $w_{k}$ 称为卷积核或滤波器，$x_{t}$ 是当前时刻产生的信号。信号序列 $x$ 和滤波器 $w$ 的卷积定义如下，其中 $*$ 表示卷积运算：

$$
y = w * x
$$

我们可以设计不同的滤波器来提取信号序列的不同特征，例如：

- 令滤波器 $w=[1 / K, \dots ,1 / K]$ 时，卷积相当于信号序列的简单移动平均
- 令滤波器 $w=[1,-2,1]$ 时，可以近似实现对信号的二阶微分

#### 二维卷积

给定一个图像 $X \in \mathbb{R}^{M \times N}$ 和一个滤波器 $W \in \mathbb{R}^{U \times V}$，一般 $U \ll M,V\ll N$，其卷积为：

$$
y_{ij}=\sum^U_{u=1} \sum^{V}_{v=1} w_{uv}x_{i-u+1, j-v+1}
$$

输入信息 $X$ 和滤波器 $W$ 的二维卷积定义为：

$$
Y = W * X
$$
其中 $*$ 表示二维卷积运算。下图给出了二维卷积示例：

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250514161608618.png?imageSlim)

- 均值滤波就是一种二维卷积
- 常用卷积作为特征提取的有效方法
- 特征映射：一副图像在经过卷积操作之后得到的结果

### 互相关

**互相关**是一个衡量两个序列相关性的函数，通常是用滑动窗口的点积计算来实现。

$$
y_{ij}=\sum^{U}_{u=1}\sum^{V}_{v=1} w_{uv}x_{i+u-1,j+v-1}
$$

互相关和卷积的区别仅仅在于**卷积核是否进行翻**转。因此互相关也可以称为**不翻转卷积**。同时上面这个公式也可以表述为 $Y = \text{rot180}(W) * X$。

> [!note]
> 在神经网络中使用卷积是为了进行特征提取，卷积核是否进行翻转和其特征提取能力无关。特别是卷积核是可学习的参数时，卷积核互相关在能力上是等价的。因此平常应用中使用互相关来代替卷积。

### 卷积的变种

- 步长 (Stride)
- 零填充 (Zero Padding)
- 常用卷积：
	- 窄卷积：步长 S=1，两端不补零 P=0，卷积后输出长度为 M-K+1.
	- 宽卷积：步长 S=1，两端补零 P=K-1，卷积后输出长度为 M+K-1.
	- 等宽卷积：步长 S=1，两端补零 (K-1)/2，卷积后输出长度为 M

### 卷积的数学性质

#### 交换性

即 $x*y=y*x$

$$
W \tilde{\otimes}X \triangleq W \otimes \tilde{X}
$$

#### 导数

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250514165839237.png?imageSlim)

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250514165900106.png?imageSlim)

## 卷积神经网络

### 用卷积代替全连接

如果使用卷积来代替全连接，在第 $l$ 层的净输入 $z^{(l)}$ 为第 $l-1$ 层活性值 $a^{(l-1)}$ 和卷积核 $w^{(l)} \in \mathbb{R}^K$ 的卷积，即：

$$
z^{(l)}=w^{l} \otimes a^{(l-1)} + b^{(l)}
$$

卷积层两个重要的性质：

- **局部链接**：在卷积层中的每一个神经元都只和前一层（第 l-1 层）中某个局部窗口内的神经元相连，构成一个局部神经网络。
- **权重共享**：作为参数的卷积核 $w^{(l)}$ 对于第 l 层的所有的神经元都是相同的。如果要提取多种特征就需要使用多个不同的卷积核。

由于以上两个性质，我们可以发现**参数个数与神经元的数量无关**。并且第 l 层的神经元个数不是任意选择的，而是满足 $M_{l}=M_{l-1}-K+1$ 。

### 卷积层

- 作用：提取一个局部区域的特征
- 不同的卷积核相当于不同的特征提取器
- 特征映射：一副图像在经过卷积提取操作之后得到的特征，每个特征映射可以作为一类抽取的图像特征。

假设一个卷积层的结构如下：

- 输入特征映射组：$\mathcal{X} \in \mathbb{R}^{M \times N \times D}$ 为三维张量
- 输出特征映射组：$\mathcal{Y} \in \mathbb{R}^{M' \times N' \times P}$
- 卷积核：$\mathcal{W} \in \mathbb{R}^{U \times V \times P \times D}$

三维结构如下图所示：

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250514172654054.png?imageSlim)

计算输出特征映射 $Y^p$，有：

$$
Z^p = W^p \otimes X + b^p = \sum^{D}_{d=1} W^{p,d} \times X^d + b^p
$$

$$
Y^p = f(Z^p)
$$

其中 $f(\cdot)$ 为非线性激活函数，一般使用 ReLU 函数。整个计算过程如下图所示：

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250514173144442.png?imageSlim)

- 参数个数：$P \times D \times ( U \times V) + P$

### 汇聚层 (Pooling Layer)

- 汇聚层 (d2l 以前的视频里称为池化层) 作用：进行特征选择，降低特征数量，减少参数数量; 有效的减少神经元的数量，还可以使得网络对一些小的局部形态保持不变性，拥有更大的感受野。

汇聚是指的对每个区域进行下采样得到一个值，作为这个区域的概括。常用的汇聚函数有两种：

- 最大汇聚：对于一个区域，选择这个区域内所有神经元的最大活性值作为这个区域的表示

$$
y^{d}_{m,n}=\underset{i \in R^d_{m,n}}{\max x_{i}}
$$

- 平均汇聚：一般是取区域内所有神经元活性值的平均值：

$$
y^d_{m,n} = \frac{1}{\mid R^d_{m,n} \mid} \sum_{i \in R^d_{m,n}}x_{i}
$$

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250514192417816.png?imageSlim)

### 卷积网络整体结构

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250514192531182.png?imageSlim)

## 参数学习

同样可以通过反向传播算法来进行参数学习

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250514192824668.png?imageSlim)

### 误差计算

- 汇聚层的误差计算

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250514193122825.png?imageSlim)

其中 $f'_{l}$ 为第 l 层使用的激活函数导数，up 为上采样函数。如果下采样是最大汇聚，误差项中每个值会直接传递到前一层对应区域中最大值所对应的神经元，该区域中其他神经元的误差项都设为 0；如果下采样是平均汇聚，误差项中每个值都会被平均分配到前一层对应区域中的所有神经元上。

- 卷积层的误差计算

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250514193152226.png?imageSlim)


## 几种典型的 CNN

这一部分请直接参考[李沐《动手学习深度学习》](https://zh.d2l.ai/chapter_convolutional-neural-networks/index.html)

## 其他卷积方式

### 转置卷积

将地位特征映射到高维特征的卷积操作称为转置卷积，也称为反卷积。

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250514194203890.png?imageSlim)

### 微步卷积

步长 S<1 的卷积操作称为微步卷积。

### 空洞卷积

是一种不增加参数数量，同时增加输出单元感受野的一种方法，也称为膨胀卷积。

卷积核有效大小为：$K'=K+(K-1) \times (D-1)$，其中 $D$ 为膨胀率