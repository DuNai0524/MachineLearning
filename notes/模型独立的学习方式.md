---
tags:
  - 深度学习
---
## 集成学习

- 集成学习就是通过多种策略将多个模型集成起来，通过群体决策来提高决策准确率。集成学习的首要问题是如何集成多个模型。 
- 比较常用的集成策略有直接平均、加权平均等。

最直接的学习策略就是直接平均，即“投票”，基于投票的集成模型 $F(x)$ 为：

$$
F(x) = \frac{1}{M} \sum^M_{m=1} f_{m}(x)
$$

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250531205041413.png?imageSlim)

加权平均有以下几类方法：

**增加模型之间的差异性**

- Bagging：通过不同模型训练数据集的独立性来提高不同模型之间的独立性。我们在原始训练集上进行有放回的随机采样，得到 M 个比较小的训练集并训练 M 个模型，然后通过投票的方式进行模型集成。
- 随机森林：在 Bagging 的基础上再引入了随机特征，进一步提高每个基模型之间的独立性。在随机森林中，每个基模型都是一颗决策树。

**Boosting 类方法**

- Boosting 类方法是按照一定的顺序来先后训练不同的的基模型，门内个模型都针对前序模型的错误进专门训练。根据前序模型的结果，来调整训练样本的权重，从而增加不同基模型之间的差异性。
- 只要基模型的准确率比随机猜测高，就可以通过集成方法来显著的提高模型的准确率。

### AdaBoost 算法

Boosting 类集成模型的目标是学习一个加性模型：

$$
F(x) = \sum^M_{m=1} \alpha_{m} f_{m}(x)
$$

其中 $f_{m}(x)$ 为弱分类器（基分类器），$\alpha_{m}$ 为弱分类器的集成权重，$F(x)$ 称为强分类器。

- Boosting 类方法的关键是如何训练每个弱分类器 $f_{m}(x)$ 以及其权重 $\alpha_{m}$
- AdaBoost 算法：采用迭代的方法，通过改变数据分布来提高弱分类器的差异，最终提高集成分类器的准确率。在每一轮训练中，增加分错样本的权重，减少分对样本的权重，从而得到一个新的数据分布。

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250531213111288.png?imageSlim)

> [!warning] 
> 关于统计学解释：看不懂怎么推的，只贴结论，推导到时候回头再回来看看

损失函数定义：

$$
\begin{equation}
\begin{split}
\mathcal{L}(F) &= \exp(-yF(x)) \\
&= \exp\left( -y \sum^M_{m=1} \alpha_{{m}} f_{m}(x) \right)
\end{split}
\end{equation}
\ \ \ \ \ \ \ y,f_{m}(x) \in \{+1, -1\}
$$

假设经过 m-1 次迭代，得到：

$$
F_{m-1}(x) = \sum^{m-1}_{t=1} \alpha_{t} f_{t}(x)
$$

则第 m 次迭代的目标是找一个 $\alpha_{m}$ 和 $f_{m}(x)$ 使得下面的损失函数最小：

$$
\mathcal{L}(\alpha_{m}, f_{m}(x)) = \sum^N_{n=1} \exp \left( -y^{(n)}(F_{m-1}(x^{(n)}) + \alpha_{m}f_{m}(x^{(n)})) \right)
$$

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250531214405296.png?imageSlim)

## 自训练和协同训练

- 半监督学习：利用少量标注数据和大量无标注数据进行学习的方法
- 两种算法：自训练和协同训练

### 自训练

自训练首先是使用标注数据来训练一个模型，并使用这个模型来预测无标注样本的标签，把预测置信度比较高的样本以及预测的伪标签加入训练集，然后重新训练新的模型，并不断重复这个过程。

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250531215521392.png?imageSlim)

- 缺点：无法爆炸每次加入的训练集的样本的伪标签是正确的
- 关键：如何设置挑选样本的标准

### 协同训练

是一种自训练的改进方式，通过两个基于不同视角的分类器来互相促进。

- 假设满足下面两个假设条件：

1. 条件独立性；使得不同视角训练处的模型就相当于从不同视角来理解问题，具有一定的互补性。
2. 充足和冗余性；当数据充分时,每种视角的特征都足以单独训练出一个正确的分类器。令 $y=g(x)$ 为需要学习的真实映射函数，$f_{1}$，$f_{2}$ 为两个视角的分类器，则有：

$$
\exists f_{1},f_{2}, \ \ \ \forall \in x, \ \ \ f_{1}(x_{1})=f_{2}(x_{2})=g(x)
$$

协同训练过程如下：

首先在训练集上根据不同视角分别训练两个模型 $f_{1}$ 和 $f_{2}$，然后用 $f_{1}$ 和 $f_{2}$ 在无标注数据集上进行预测，各选取预测置信度比较高的样本加入训练集，重新训练两个不同视角的模型，并不断重复这个过程。

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250531221009055.png?imageSlim)

> [!note]
> 如果两种视角完全一样，则退化成自训练算法

## 多任务学习

一般的机器学习模型都是针对单一的特定任务，不同任务的模型都是在各自的训练集上单独学习得到的。如果有两个任务比较相关，它们之间会存在一定的共享知识。目前，主流的多任务学习方法主要关注表示层面的共享。

- 多任务学习：同时学习多个相关任务，让这些任务在学习过程中共享知识，利用多个任务之间的相关性来改进模型在每个任务上的性能和泛化能力。
	- 可以看作一种归纳迁移学习

**共享机制**

多任务学习的主要挑战是在于如何设计多任务之间的共享机制。在神经网络模型中，模型共享变得相对容易。深度神经网络模型提供了一种很方便的信息共享方式，可以很容易地进行多任务学习。共享模式如下图所示：

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250531222335856.png?imageSlim)

1. 硬共享模式：让不同任务的神经网络模型共同使用一些共享模块来提取一些通用特征，然后再针对每个不同多个任务设置一些私有模块来提取一些特征任务的特征 (有点像迁移学习？)
2. 软共享模式：不显式地设置共享模块，但每个任务都可以从其他任务中“窃取”一些信息来提高自己的能力。窃取的方式包括直接复制使用其他任务的隐状态，或者使用注意力机制来主动使用有用的信息。
3. 层次共享模式：一般神经网络中不同层抽取的特征类型不同，底层一般抽取一些低级的局部特征，高层抽取一些高级的语义特征。因此如果多任务学习中不同任务也有级别高低之分，那么一个合理的共享模式是让低级任务在底层输出，高级任务在高层输出。
4. 共享-私有模式：一个更加分工明确的方式是将共享模块和任务特定模块的责任分开。共享模块捕捉一些跨任务的共享特征，而私有模块捕捉和特定任务相关的特征，最终的表示由私有特征和共享特征组成。

**学习步骤**

在多学习任务重，每个任务都可以有自己单独的训练集。为了让所有任务同时学习，我们通常使用交替训练的方法来“近似”地实现同时学习。其可以分为两个阶段：

1. 联合训练阶段：每次迭代时，随机选取一个任务，然后从这个任务中选择一些训练样本，计算梯度并更新参数。

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250531223656600.png?imageSlim)

1. 单任务精调阶段：基于多任务学习得到的参数，分别在每个单独任务进行精调。

多任务学习通常可以获得比单任务学习更好的泛化能力，原因在于：

1. 多任务学习在多个任务的数据集上进行训练，训练集更大。并且多个任务之间有一定关联，因此是一种隐式的数据增强
2. 可以看作一种正则化
3. 多任务学习的机制会使得它回避单任务学习获得更好的表示
4. 每个人物都可以“选择性”利用他其他任务中学习到的隐藏特征，从而提高自身的能力

## 迁移学习

- 迁移学习需要解决的问题：如何将相关任务的训练数据中的可泛化知识迁移到目标任务上
- 迁移学习：指的是两个不同领域的知识迁移问题，利用源领域中学到的知识来帮助目标领域上的学习任务，源领域的训练样本数量一般大于目标领域
- 根据不同的迁移方式又可以分为两个类型：归纳迁移学习和转导迁移学习
- 归纳迁移学习：在源领域和任务上学习出一般的规律，然后将这个规律迁移到目标领域和任务上。
- 转导迁移学习：从样本到样本的迁移，直接用源领域和目标领域的样本进行迁移学习。

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250601111226929.png?imageSlim)

- 对于两个机器学习的范式：归纳学习和转导学习。一般的机器学习都是指的是归纳学习，即希望在训练集上学习得到期望风险最小的模型。而转导学习的目标是学习一种正在给定测试集上错误率最小的模型，在训练阶段可以利用测试集的信息。

### 归纳迁移学习

- 源领域和目标领域有相同的输出空间，输入空间可以相同也可以不同，源任务和目标任务一般不相同。
- 一般而言，归纳迁移学习要求源领域和目标领域是相关的，并且源领域有大量的训练样本，这些样本可以有标注的样本，也可以是无标注的样本。
- 根据目标任务自身的特点以及源任务的相关性，可以有针对性地选择预训练模型的不同层来迁移到目标任务中。

1. 当源领域只有大量无标注数据时，源任务可以转化为无监督学习任务，比如自编码和密度估计任务。通过一些无监督学习任务学习一种可以迁移的表示，然后在将这种表示迁移到目标任务上。
2. 当源领域有大量的标注数据的时候，可以直接将源领域上训练的模型迁移到目标领域上。

归纳迁移学习一般有以下两种迁移方式：

1. 基于特征的方式：将预训练模型的输出或者是中间隐藏层输出作为特征直接机制入道目标任务的学习模型中，
2. 精调的方式：在目标任务上复用预训练模型的部分组件，并对其参数进行精调。

预训练模型迁移通常比从零开始学习方式更好，主要有以下两点：

1. 初始模型性能一般比随机初始化的模型好
2. 训练时模型的学习速度比从零开始快，收敛性更好
3. 模型的最终性能更好，具有更好的泛化性

归纳迁移和多任务学习比较类似，但是有两点区别：

1. 多任务学习是同时学习两个不同的任务，归纳迁移学习通常有两个阶段
2. 归纳迁移学习是单向的知识迁移，希望提高模型在目标任务上的性能；多任务学习是希望提高所有任务的性能。

### 转导迁移学习

- 转导迁移学习是一种样本到样本的迁移，直接利用源领域和目标领域的样本进行学习。
- 通常假设源领域有大量的标注数据，而目标领域没有（或只有少量）标注数据，但是有大量的无标注数据。
- 一个常见子问题是领域适应，一般假设源领域和目标领域有相同的样本空间，但是数据分布不同，一般有三种情况：

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250601114519255.png?imageSlim)

目前大多数领域关注协变量偏移，这样领域适应问题的关键就在于如何学习领域无关的表示。

> [!note] 关于协变量:
> ![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250601114622986.png?imageSlim)

假设 $p_{S}(y|x) = p_{T}(y|x)$，则领域适应的目标是学习一个模型 $f:\mathcal{X \to Y}$ 使得：

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250601115732543.png?imageSlim)

如果我们可以学习一个映射函数 $g:\mathcal{X} \to \mathbb{R}^d$，将 x 映射到一个特征空间中，并在这个特征中间中使得源领域和目标领域的编辑分布相同，那么目标函数可以近似为：

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250601120357945.png?imageSlim)

学习的目标是优化参数 $\theta_{f},\theta_{g}$ 使得提取的特征是领域无关的，并且在源领域上损失最小。

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250601124022433.png?imageSlim)

## 终身学习

- 终身学习：也叫持续学习，是指像人类一样具有持续不断的学习能力，根据历史任务中学到的经验和知识来帮助学习不断出现的新任务，并且这些经验和知识是持续累积的，不会因为新的任务而忘记旧的知识。
- 一个关键的问题是如何避免灾难性遗忘，及按照一定顺序学习多个任务时，学习新任务的同时不忘记之前学习过的任务。
- 可以使用弹性权重巩固来解决灾难性遗忘。

## 元学习

- 元学习：面对一个新的任务，动态调整学习方式的能力
- 目的：从已有任务中学习一种学习方法或者元知识，可以加速新任务的学习。类似于归纳迁移学习，但是元学习更加侧重从多种不同的任务中归纳出一种学习方法。
- 小样本学习：每个类只有 K 个标注样本，K=1 为单样本，K=0 为零样本。

### 基于优化器的元学习

通过另一个神经网络来建模梯度下降的过程：

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250601152307178.png?imageSlim)

使用函数 $g_{t}(\cdot)$ 来预测第 t 步的时候参数更新的差值 $\Delta \theta_{t} = \theta_{t} - \theta_{t-1}$，其中函数 $g(\cdot)$ 称为优化器，输入的是当前时刻的梯度值，输出的是参算数的更新差值。第 t 步的更新规则可以写为：

$$
\theta_{t+1} = \theta_{t} + g_{t}(\nabla \mathcal{L}(\theta_{t});\phi)
$$

学习优化器的过程可以看作一种元学习过程，其目标是找到一个适用于多个不同任务的优化器，具体的目标函数为：

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250601152925424.png?imageSlim)

其中 T 为最大迭代次数，$w_{t} > 0$ 为每一步的权重，一般可以设置为 $w_{t}=1$。由于 LSTM 网络可以记忆梯度的历史信息，学习到的优化器可以看作一个高阶的优化方法。

- 一种简化的方法：为每个参数都使用一个共享的 LSTM 网络来进行更新，这样可以使用一个非常小的 LSTM 网络来更新参数

### 模型无关的元学习

- 模型无关的元学习：假设所有的任务都来自于一个任务空间，我们可以在这个任务空间的所有任务上学习一种通用的表示，这种表示可以经过梯度下降方法在一个特定的单任务上进行精调。

假设一个模型为 $f_{\theta}$，如果我们让这个模型适配到一个新任务上，通过异步或者多步的梯度下降更新，学习到的任务适配参数为：

$$
\theta'_{m}=\theta - \alpha \nabla_{\theta} \mathcal{L}_{\mathcal{T}_{m}}(f_{\theta})
$$

MAML 的目标是学习一个参数 $\theta$ 使得其经过一个梯度迭代就可以在新任务上达到最好的性能：

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250601154408346.png?imageSlim)

在所有任务上源优化也采用梯度下降来进行优化，即：

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250601154434268.png?imageSlim)

- 当 $\alpha$ 比较小时，MAML 就近似为普通的多任务学习优化方法。

![image.png|500](https://dunaifujian-1308176953.cos.ap-guangzhou.myqcloud.com/20250601153759871.png?imageSlim)
